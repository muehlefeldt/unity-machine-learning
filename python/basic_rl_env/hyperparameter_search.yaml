# Default values for userconfig: All bool values set to true.

userconfig:
  # Use build env? Set false for run using the unity editor.
  # Attention: This requieres a manually pre build by the user using the editor if set to true.
  # If true: sensor count and stat file export is used. See option below.
  build: true

  # Production mode? Bool requiered. Set to false if ml-agents is not to be run.
  production: true

  # "true" to generate summary file.
  summary: true

  # Set to false to delete all files after the programm finishes.
  keep_files: true

  # Initialize from existing previous run.
  not_based_on_previous_nn: true

  # Provide run id of previous NN to be used for new run.
  # Only used if not_based_on_previous_nn: false is used.
  previous_run_id: 1410

  # Specify number of parallel env to use. Default = 1.
  num_env: 1

  # Attention: Do not change, issue with the unity config. Number of parallel processes. Default = 1.
  num_process: 1

  # Provide message for the log file for better traceability.
  message: "See log file."

  # Random excution order of generated hyperparameter configs.
  execution_order_not_random: true

# Set unity enviroment specific options.
# Contains all settings directly passed to the unity environemt via a JSON config file loaded
# during inital loading of the Unity environment.
env_config:
  # Define number of horizontal sensors. Can be single int or a list of int given as [1, 2].
  # Schould be between 1 and 32, but higher is possible.
  sensorCount: 32

  # Deploy a decoy object to increase complexity of the environment. Default set to false.
  useDecoy: false

  # Create a inner wall separating two rooms. Default set to true.
  createWall: true

  # Width of the created door as float. Default set to 4.0.
  doorWidth: 4.0

  # Use random position of the inner wall. Both rooms are of varying size. Default set to true.
  randomWallPosition: true
    
  # Random position of the door within the inner wall. Default set to true.
  randomDoorPosition: true
    
  # Select if target and agent are always in different rooms. Default set to false.
  targetAlwaysInOtherRoomFromAgent: false
    
  # Fix the position of the target. Position will be hard coded.
  targetFixedPosition: false

  # Set the number of time steps in unity per episode. This equals 
  maxStep: 1000

  # Penalty (negative reward) given for every taken step. Assume as default -1 / maxStep.
  stepPenalty: -0.0005

# Paths used by the python script to generate, save and load files.
paths:
  working_dir: "./python/basic_rl_env"

  # If a pre-build Unity environment is used, provided here path to the enviroment.
  unity_env: "C:/build/windows"
  
  # Data folder in the build env dir. Config file for the env is stored and loaded here.
  unity_env_data: "C:/build/windows/basic_rl_environment_Data"

  # Need to save env config file to the editor, if editor is used.
  unity_assets: "C:/Users/max.muehlefeldt/Documents/GitHub/unity-machine-learning/unity/basic_rl_environment/Assets"

  # Where to store the log files generated by the script.
  log_dir: "./logs"

  # Python script generates after the ml-agents runs a summary file.
  summaries_dir: "./summaries"

  # Dir where ml-agents saves the results of the runs. Same dir is accessed by tensorboard.
  results_dir: "./results"

  # Dir to store old results of ml-agents. Dirs from results_dir can be moved by hand here.
  results_archive_dir: "./results_archive"

  # Dir to store configs for ml-agents.
  configs_dir: "./configs"

  # Dir to store the configuration files loaded by unity to configure the env.
  unity_configs_dir: "./unity_configs"

  # Define path of the stats file to be exported at the end of the ml-agents run.
  # Stats file is created by Unity code and provides more detail about the run.
  # Naming convention needs to be so. Do not change.
  # Path is added to the Unity env config file.
  statsExportPath: "./stats"



# ML-Agents configuration.
behaviors:
  RollerAgent:
    # Common Trainer Configurations.
    trainer_type: ppo
    keep_checkpoints: 5
    max_steps: 100e6
    time_horizon: 2000
    summary_freq: 10000

    hyperparameters:
      # Common Trainer Configurations.
      batch_size: 356
      buffer_size: 51200
      learning_rate: 1e-2
      learning_rate_schedule: constant

      # PPO-specific Configurations.
      beta: 1e-2
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      
    network_settings:
      conditioning_type: none # Can be none, because no goal observation is used.
      normalize: false
      hidden_units: 512
      num_layers: 1
      # Remember: sequence_len <= batch_size.
      memory:
        memory_size: 16
        sequence_length: 4
      #vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      #curiosity:
      #  strength: 1.0
      #  gamma: 0.99
      #  learning_rate: 3e-4
      #  network_settings: 
      #   hidden_units: 128
