{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import scipy.constants as sc\n",
    "from scipy.signal import savgol_filter\n",
    "#from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "#plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    CUMULATIVE_REWARD = \"cumulative_rewards\"\n",
    "    EP_LENGTH = \"ep_length\"\n",
    "    DOOR_PASSAGE = \"passage\"\n",
    "    COLLISION_INITIAL = \"initial\"\n",
    "    COLLISION_STAY = \"stay\"\n",
    "    COLLISION_TOTAL = \"total\"\n",
    "    DOOR_GOOD = \"good_passage\"\n",
    "    DOOR_BAD = \"bad_passage\"\n",
    "\n",
    "class Plot(Enum):\n",
    "    COLLISION = 0\n",
    "    ENV1 = 1\n",
    "    ENV2 = 2\n",
    "    LINEPLOT = 3\n",
    "    RESULT_DATA_ENV1 = 4\n",
    "    RESULT_DATA_ENV2 = 5\n",
    "    DOOR = 6\n",
    "    DOOR_HIST = 7\n",
    "    SENSOR_STUDY_EP_LENGTH = 8\n",
    "    SENSOR_STUDY_REWARD = 9\n",
    "    RESULT_DATA_SENSOR_STUDY = 10\n",
    "\n",
    "tag_colors = {\n",
    "    Tag.CUMULATIVE_REWARD: \"C1\",\n",
    "    Tag.EP_LENGTH: \"C2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from what run to get data. Also choose the Type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_runs = True\n",
    "selected_runs: list[int] =  list(range(6072, 6081)) #[6072]\n",
    "plot_step = 5\n",
    "plot_all = True\n",
    "#selected_tag = Tag.EP_LENGTH\n",
    "selected_plot = Plot.RESULT_DATA_SENSOR_STUDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get y data fro the summary dict using the ID of the training run.\n",
    "The tag specifies the selected type of data.\"\"\"\n",
    "def get_y_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    try:\n",
    "        if tag in [Tag.CUMULATIVE_REWARD, Tag.EP_LENGTH]:\n",
    "            return summary_dict[id][\"env\"][tag.value]\n",
    "        if tag in [Tag.DOOR_PASSAGE, Tag.DOOR_BAD, Tag.DOOR_GOOD]:\n",
    "            return summary_dict[id][\"door\"][tag.value]\n",
    "        if tag in [Tag.COLLISION_STAY, Tag.COLLISION_INITIAL, Tag.COLLISION_STAY]:\n",
    "            return summary_dict[id][\"collision\"][tag.value]\n",
    "    except ValueError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get x data from the summary dict. Is based on the recorded step.\"\"\"\n",
    "def get_x_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    d = len(get_y_data(summary_dict, id, tag))\n",
    "    return summary_dict[id][\"steps\"][:d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter data to smooth plots.\"\"\"\n",
    "def filter_data(data):\n",
    "    try: \n",
    "        return savgol_filter(data, 15, 2)\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_arrays_to_same_length(a: np.ndarray, b:np.ndarray):\n",
    "    if len(a) != len(b):\n",
    "        if len(a) < len(b):\n",
    "            d = len(a)\n",
    "            b = b[:d]\n",
    "        else:\n",
    "            d = len(b)\n",
    "            a = a[:d]\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle with all the run data.\n",
    "summary_file_path = Path(\"C:/Users/max.muehlefeldt/Documents/GitHub/unity-machine-learning/python/basic_rl_env/summary_dict.pickle\").absolute()\n",
    "\n",
    "with open(summary_file_path, mode=\"rb\") as file:\n",
    "    summary_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "matplotlib.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [Plot.SENSOR_STUDY_EP_LENGTH, Plot.SENSOR_STUDY_REWARD]:\n",
    "    selected_tag = Tag.CUMULATIVE_REWARD\n",
    "    if selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        \n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag]\n",
    "            label=run_label\n",
    "            )\n",
    "    \n",
    "    \n",
    "    if selected_plot is Plot.SENSOR_STUDY_REWARD:\n",
    "        axs_first.set_ylim(-2, 1.6)\n",
    "        axs_first.axhline(1.5, label=\"Reward limit = 1.5\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "        axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    #elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "    #    axs_first.set_ylim(-1, 1)\n",
    "    #    axs_first.set_ylabel(\"Passage Quality\")\n",
    "    #    axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()   \n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.COLLISION:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        #y = savgol_filter( get_y_data(summary_dict, id, selected_tag), 15, 2)\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        #axs_first.plot(\n",
    "        #    x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag]\n",
    "        #    label=run_label\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        y_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "\n",
    "        if len(y_initial) != len(y_stay):\n",
    "            if len(y_initial) < len(y_stay):\n",
    "                d = len(y_initial)\n",
    "                y_stay = y_stay[:d]\n",
    "                x = x[:d]\n",
    "            else:\n",
    "                d = len(y_stay)\n",
    "                y_initial = y_initial[:d]\n",
    "                x = x[:d]\n",
    "        \n",
    "        y = np.vstack([y_initial, y_stay])\n",
    "\n",
    "        \n",
    "\n",
    "        axs_first.stackplot(\n",
    "            x, y,\n",
    "            labels=[\"Initial contact\", \"Continued contact\"],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        axs_first.set_ylim(0, 200)\n",
    "    #if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "    #    #axs_first.set_ylim(-2, 1.5)\n",
    "    #    axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "    #    axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Collisions\")\n",
    "\n",
    "    #elif selected_tag is Tag.EP_LENGTH:\n",
    "    #    axs_first.set_ylim(0, 200)\n",
    "    #    axs_first.set_ylabel(\"Length\")\n",
    "    #    axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    #elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "    #    axs_first.set_ylim(-1, 1)\n",
    "    #    axs_first.set_ylabel(\"Passage Quality\")\n",
    "    #    axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "        axs_first.set_xlabel(\"Step\")\n",
    "        \n",
    "        axs_first.legend(title=f\"Run {id}\")\n",
    "        axs_first.grid()   \n",
    "\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        plt.style.use(\"default\")\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "        fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [Plot.ENV1, Plot.ENV2]:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "\n",
    "        run_label = f\"Cumulutive reward\"\n",
    "        selected_tag = Tag.CUMULATIVE_REWARD\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Cumulative reward\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_first.set_ylim(\n",
    "            -2, \n",
    "            1.1 if selected_plot is Plot.ENV1 else 1.6\n",
    "        )\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        reward_label = \"Reward limit = 1.0\" if selected_plot is Plot.ENV1 else  \"Reward limit = 1.5\"\n",
    "        axs_first.axhline(\n",
    "            1.0 if selected_plot is Plot.ENV1 else 1.5,\n",
    "            label=reward_label, color=\"C3\", linestyle=\"dashed\"\n",
    "        )\n",
    "\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        axs_second = axs_first.twinx()\n",
    "        axs_second.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Episode length\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_second.set_ylim(0, 200)\n",
    "        axs_second.set_ylabel(\"Episode Length\")\n",
    "        #fig.suptitle(\"Cumulative reward and episode length\")\n",
    "    \n",
    "    \"\"\"if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "        #axs_first.set_ylim(-2, 1.5)\n",
    "        axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_tag is Tag.EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\"\"\"\n",
    "    axs_first.set_title(\"Cumulative reward and episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_second.legend()\n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_second.grid()\n",
    "    #axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        y = -1 * y\n",
    "\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag],\n",
    "            label=\"Incorrect passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "       \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=\"Correct passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    \n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR_HIST:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_bad = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #y = -1 * y\n",
    "\n",
    "\n",
    "        #axs_first.hist(\n",
    "        #    #x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag],\n",
    "        #    label=\"Incorrect passage\",\n",
    "        #    linewidth=2.0,\n",
    "        #    )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_good = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "\n",
    "        y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "\n",
    "        # Use a constant bin width to make the two histograms easier to compare visually\n",
    "        #bin_width = 1\n",
    "        #bins = np.arange(np.min([y_bad, y_good]),\n",
    "                            #np.max([y_bad, y_good]) + bin_width, bin_width)\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_bad,\n",
    "            weights=-np.ones_like(y_bad),\n",
    "            #bins=bins,\n",
    "            label=\"Inorrect passage\",\n",
    "            )\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_good,\n",
    "            #bins=bins,\n",
    "            label=\"Correct passage\",\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    axs_first.axhline(0, color=\"k\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result data\n",
    "To highlight the final performance of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV1:\n",
    "    for id in selected_runs:\n",
    "        y_collision_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_collision_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "        y_cumulative_reward = filter_data(get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD))\n",
    "        y_ep_length = filter_data(get_y_data(summary_dict, id, Tag.EP_LENGTH))\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        print(f\"Episode length & {np.round(final_ep_length, 3)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_quality(x, y):\n",
    "    return x / (x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV2:\n",
    "    for id in selected_runs:\n",
    "        y_collision_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_collision_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "        y_cumulative_reward = filter_data(get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD))\n",
    "        y_ep_length = filter_data(get_y_data(summary_dict, id, Tag.EP_LENGTH))\n",
    "        y_door_passage_incorrect = filter_data(get_y_data(summary_dict, id, Tag.DOOR_BAD))\n",
    "        y_door_passage_correct = filter_data(get_y_data(summary_dict, id, Tag.DOOR_GOOD))\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-10:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-10:])))\n",
    "        print(f\"Door passage quality $[-1, 1]$ & {np.round(door_passage_quality, 3)} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hline\n",
      "Indicator / Sensors & 1 & 2 & 4 & 8 & 10 & 16 & 32 & 64 & 128 \\\\\n",
      "\\hline\n",
      "Cumulative reward & -0.482 & -0.447 & -0.05 & -0.003 & 0.039 & 0.56 & 0.574 & 0.43 & 0.806 \\\\\n",
      "Cumulative reward STD & 0.352 & 0.218 & 0.124 & 0.113 & 0.128 & 0.103 & 0.096 & 0.151 & 0.062 \\\\\n",
      "Episode length & 187.34 & 176.64 & 148.71 & 140.18 & 140.11 & 111.28 & 111.1 & 112.43 & 92.71 \\\\\n",
      "Collision rate & nan & nan & nan & nan & nan & nan & nan & nan & nan \\\\\n",
      "Door passage quality & 0.561 & 0.485 & 0.59 & 0.581 & 0.6 & 0.694 & 0.678 & 0.726 & 0.75 \\\\\n",
      "\\hline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\max.muehlefeldt\\Documents\\GitHub\\unity-machine-learning\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\max.muehlefeldt\\Documents\\GitHub\\unity-machine-learning\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_SENSOR_STUDY:\n",
    "    run_data = {}\n",
    "\n",
    "    for id in selected_runs:\n",
    "        run_data[id] = {}\n",
    "        y_collision_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_collision_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "        y_cumulative_reward = filter_data(get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD))\n",
    "        y_ep_length = filter_data(get_y_data(summary_dict, id, Tag.EP_LENGTH))\n",
    "        y_door_passage_incorrect = filter_data(get_y_data(summary_dict, id, Tag.DOOR_BAD))\n",
    "        y_door_passage_correct = filter_data(get_y_data(summary_dict, id, Tag.DOOR_GOOD))\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[id][\"final_mean_reward\"]\n",
    "        #print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"final_cumulative_reward\"] = np.round(final_cumulative_reward, 3)\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[id][\"final_std_reward\"]\n",
    "        #print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"final_cumulative_reward_std\"] = np.round(final_cumulative_reward_std, 3)\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        #print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "        run_data[id][\"final_ep_length\"] = np.round(np.round(final_ep_length, 2), 3)\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_collision_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        #print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "        run_data[id][\"final_collision_rate\"] = np.round(final_collision_rate, 3)\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        #print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "        run_data[id][\"performed_steps\"] = performed_steps\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-10:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-10:])))\n",
    "        #print(f\"Door passage quality $[-1, 1]$ & {np.round(door_passage_quality, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"door_passage_quality\"] = np.round(door_passage_quality, 3)\n",
    "\n",
    "        run_data[id][\"sensor_count\"] = summary_dict[id][\"file_contents\"]['stats']['sensorCount']\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    s = f\"Indicator / Sensors & \"\n",
    "    for id in selected_runs:\n",
    "        s+= f\"{run_data[id]['sensor_count']} & \"\n",
    "    s = s[:-2] + \"\\\\\\\\\"\n",
    "    print(s)\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    performance_indicators = [\n",
    "        (\"Sensors\", \"sensor_count\"),\n",
    "        (\"Cumulative reward\", \"final_cumulative_reward\"),\n",
    "        (\"Cumulative reward STD\", \"final_cumulative_reward_std\"),\n",
    "        (\"Episode length\", \"final_ep_length\"),\n",
    "        (\"Collision rate\", \"final_collision_rate\"),\n",
    "        (\"Door passage quality\",\"door_passage_quality\")\n",
    "    ]\n",
    "    for indicator in performance_indicators:\n",
    "        if indicator == (\"Sensors\", \"sensor_count\"):\n",
    "            continue\n",
    "        s = f\"{indicator[0]} & \"\n",
    "\n",
    "        for id in run_data.keys():\n",
    "            s += f\"{run_data[id][indicator[1]]} & \"\n",
    "        s = s[:-2] + \"\\\\\\\\\"\n",
    "        print(s)\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'& '"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = f\"Indicator / Run & \"\n",
    "s[-2:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b827e17ac296308a841e2dc6cc282cd5ca71c13fcd532c5acf2c55a086afa311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
