{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import scipy.constants as sc\n",
    "from scipy.signal import savgol_filter\n",
    "#from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "#plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    CUMULATIVE_REWARD = \"cumulative_rewards\"\n",
    "    EP_LENGTH = \"ep_length\"\n",
    "    DOOR_PASSAGE = \"passage\"\n",
    "    COLLISION_INITIAL = \"initial\"\n",
    "    COLLISION_STAY = \"stay\"\n",
    "    COLLISION_TOTAL = \"total\"\n",
    "    DOOR_GOOD = \"good_passage\"\n",
    "    DOOR_BAD = \"bad_passage\"\n",
    "    TARGET_REACHED = \"target_reached\"\n",
    "\n",
    "class Plot(Enum):\n",
    "    COLLISION = 0\n",
    "    ENV1 = 1\n",
    "    ENV2 = 2\n",
    "    LINEPLOT = 3\n",
    "    RESULT_DATA_ENV1 = 4\n",
    "    RESULT_DATA_ENV2 = 5\n",
    "    DOOR = 6\n",
    "    DOOR_HIST = 7\n",
    "    SENSOR_STUDY_EP_LENGTH = 8\n",
    "    SENSOR_STUDY_REWARD = 9\n",
    "    SENSOR_STUDY_DOOR_GOOD = 10,\n",
    "    SENSOR_STUDY_DOOR_BAD = 11,\n",
    "    SENSOR_STUDY_INITIAL_COLLISION = 12\n",
    "    SENSOR_STUDY_CONTINUED_COLLISION = 13\n",
    "    SENSOR_STUDY_DOOR_QUALITY = 14\n",
    "    RESULT_DATA_SENSOR_STUDY = 15\n",
    "    STEP_PENALTY_EP_LENGTH = 16\n",
    "\n",
    "tag_colors = {\n",
    "    Tag.CUMULATIVE_REWARD: \"C1\",\n",
    "    Tag.EP_LENGTH: \"C2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from what run to get data. Also choose the Type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_runs = True\n",
    "selected_runs: list[int] = [6465]#list(range(6466, 6475))# [6465]#list(range(6475, 6480)) # list(range(6466, 6475)) #[6072]6466 - 6474\n",
    "plot_step = 5\n",
    "plot_all = True\n",
    "\n",
    "# Select the plot you want:\n",
    "selected_plot = Plot.RESULT_DATA_ENV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get y data fro the summary dict using the ID of the training run.\n",
    "The tag specifies the selected type of data.\"\"\"\n",
    "def get_y_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    try:\n",
    "        if tag in [Tag.CUMULATIVE_REWARD, Tag.EP_LENGTH, Tag.TARGET_REACHED]:\n",
    "            return summary_dict[id][\"env\"][tag.value]\n",
    "        if tag in [Tag.DOOR_PASSAGE, Tag.DOOR_BAD, Tag.DOOR_GOOD]:\n",
    "            return summary_dict[id][\"door\"][tag.value]\n",
    "        if tag in [Tag.COLLISION_STAY, Tag.COLLISION_INITIAL, Tag.COLLISION_STAY]:\n",
    "            return summary_dict[id][\"collision\"][tag.value]\n",
    "    except ValueError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get x data from the summary dict. Is based on the recorded step.\"\"\"\n",
    "def get_x_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    d = len(get_y_data(summary_dict, id, tag))\n",
    "    return summary_dict[id][\"steps\"][:d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter data to smooth plots.\"\"\"\n",
    "def filter_data(data):\n",
    "    try: \n",
    "        return savgol_filter(data, 15, 2)\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_arrays_to_same_length(a: np.ndarray, b:np.ndarray):\n",
    "    if len(a) != len(b):\n",
    "        if len(a) < len(b):\n",
    "            d = len(a)\n",
    "            b = b[:d]\n",
    "        else:\n",
    "            d = len(b)\n",
    "            a = a[:d]\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_quality(good, bad):\n",
    "    x = good\n",
    "    y = bad\n",
    "    try: \n",
    "        result = x / (x + y)\n",
    "    except RuntimeWarning:\n",
    "        result = 0.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle with all the run data.\n",
    "summary_file_path = Path(\"C:/Users/max.muehlefeldt/Documents/GitHub/unity-machine-learning/python/basic_rl_env/summary_dict.pickle\").absolute()\n",
    "\n",
    "with open(summary_file_path, mode=\"rb\") as file:\n",
    "    summary_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "matplotlib.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [\n",
    "    Plot.SENSOR_STUDY_EP_LENGTH, \n",
    "    Plot.SENSOR_STUDY_REWARD,\n",
    "    Plot.SENSOR_STUDY_INITIAL_COLLISION,\n",
    "    Plot.SENSOR_STUDY_CONTINUED_COLLISION,\n",
    "    Plot.SENSOR_STUDY_DOOR_GOOD,\n",
    "    Plot.SENSOR_STUDY_DOOR_BAD,\n",
    "    Plot.SENSOR_STUDY_DOOR_QUALITY\n",
    "    ]:\n",
    "    selected_tag = Tag.CUMULATIVE_REWARD\n",
    "    if selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_INITIAL_COLLISION:\n",
    "        selected_tag = Tag.COLLISION_INITIAL\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_CONTINUED_COLLISION:\n",
    "        selected_tag = Tag.COLLISION_STAY\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_GOOD:\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_BAD:\n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_QUALITY:\n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        \n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        \n",
    "        if selected_plot is Plot.SENSOR_STUDY_DOOR_QUALITY:\n",
    "            selected_tag = Tag.DOOR_BAD\n",
    "            y_bad = -1 * np.array(get_y_data(summary_dict, id, selected_tag))\n",
    "            selected_tag = Tag.DOOR_GOOD\n",
    "            y_good = np.array(get_y_data(summary_dict, id, selected_tag))\n",
    "            y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "            #y = []\n",
    "            #for index in range(len(y_good)):\n",
    "                #y.append(get_passage_quality(y_good[index], y_bad[index]))\n",
    "            y = get_passage_quality(y_good, y_bad)\n",
    "        elif selected_plot is Plot.SENSOR_STUDY_CONTINUED_COLLISION:\n",
    "            y = filter_data(filter_data(filter_data(y)))\n",
    "        elif selected_plot is Plot.SENSOR_STUDY_DOOR_BAD:\n",
    "            y = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        else:\n",
    "            y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag]\n",
    "            label=run_label\n",
    "            )\n",
    "    \n",
    "    \n",
    "    if selected_plot is Plot.SENSOR_STUDY_REWARD:\n",
    "        axs_first.set_ylim(-2, 1.6)\n",
    "        axs_first.axhline(1.5, label=\"Reward limit = 1.5\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "        axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    elif selected_plot is Plot.SENSOR_STUDY_INITIAL_COLLISION:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Initial Collisions\")\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_CONTINUED_COLLISION:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Continued Collisions\")\n",
    "    \n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_GOOD:\n",
    "        axs_first.set_title(\"Correct Door Passages\")\n",
    "        axs_first.set_ylim(0, 95)\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_BAD:\n",
    "        axs_first.set_title(\"Incorrect Door Passages\")\n",
    "        axs_first.set_ylim(0, 95)\n",
    "        \n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()   \n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.COLLISION:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        #y = savgol_filter( get_y_data(summary_dict, id, selected_tag), 15, 2)\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        #axs_first.plot(\n",
    "        #    x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag]\n",
    "        #    label=run_label\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        y_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "\n",
    "        if len(y_initial) != len(y_stay):\n",
    "            if len(y_initial) < len(y_stay):\n",
    "                d = len(y_initial)\n",
    "                y_stay = y_stay[:d]\n",
    "                x = x[:d]\n",
    "            else:\n",
    "                d = len(y_stay)\n",
    "                y_initial = y_initial[:d]\n",
    "                x = x[:d]\n",
    "        \n",
    "        y = np.vstack([y_initial, y_stay])\n",
    "\n",
    "        \n",
    "\n",
    "        axs_first.stackplot(\n",
    "            x, y,\n",
    "            labels=[\"Initial contact\", \"Continued contact\"],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        axs_first.set_ylim(0, 200)\n",
    "    #if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "    #    #axs_first.set_ylim(-2, 1.5)\n",
    "    #    axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "    #    axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Collisions\")\n",
    "\n",
    "    #elif selected_tag is Tag.EP_LENGTH:\n",
    "    #    axs_first.set_ylim(0, 200)\n",
    "    #    axs_first.set_ylabel(\"Length\")\n",
    "    #    axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    #elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "    #    axs_first.set_ylim(-1, 1)\n",
    "    #    axs_first.set_ylabel(\"Passage Quality\")\n",
    "    #    axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "        axs_first.set_xlabel(\"Step\")\n",
    "        \n",
    "        axs_first.legend(title=f\"Run {id}\")\n",
    "        axs_first.grid()   \n",
    "\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        plt.style.use(\"default\")\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "        fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [Plot.ENV1, Plot.ENV2]:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "\n",
    "        run_label = f\"Cumulutive reward\"\n",
    "        selected_tag = Tag.CUMULATIVE_REWARD\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Cumulative reward\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_first.set_ylim(\n",
    "            -2, \n",
    "            1.1 if selected_plot is Plot.ENV1 else 1.6\n",
    "        )\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        reward_label = \"Reward limit = 1.0\" if selected_plot is Plot.ENV1 else  \"Reward limit = 1.5\"\n",
    "        axs_first.axhline(\n",
    "            1.0 if selected_plot is Plot.ENV1 else 1.5,\n",
    "            label=reward_label, color=\"C3\", linestyle=\"dashed\"\n",
    "        )\n",
    "\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        axs_second = axs_first.twinx()\n",
    "        axs_second.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Episode length\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_second.set_ylim(0, 200)\n",
    "        axs_second.set_ylabel(\"Episode Length\")\n",
    "        #fig.suptitle(\"Cumulative reward and episode length\")\n",
    "    \n",
    "    \"\"\"if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "        #axs_first.set_ylim(-2, 1.5)\n",
    "        axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_tag is Tag.EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\"\"\"\n",
    "    axs_first.set_title(\"Cumulative reward and episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_second.legend()\n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_second.grid()\n",
    "    #axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        y = -1 * y\n",
    "\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag],\n",
    "            label=\"Incorrect passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "       \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=\"Correct passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    \n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR_HIST:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_bad = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #y = -1 * y\n",
    "\n",
    "\n",
    "        #axs_first.hist(\n",
    "        #    #x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag],\n",
    "        #    label=\"Incorrect passage\",\n",
    "        #    linewidth=2.0,\n",
    "        #    )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_good = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "\n",
    "        y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "\n",
    "        # Use a constant bin width to make the two histograms easier to compare visually\n",
    "        #bin_width = 1\n",
    "        #bins = np.arange(np.min([y_bad, y_good]),\n",
    "                            #np.max([y_bad, y_good]) + bin_width, bin_width)\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_bad,\n",
    "            weights=-np.ones_like(y_bad),\n",
    "            #bins=bins,\n",
    "            label=\"Inorrect passage\",\n",
    "            )\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_good,\n",
    "            #bins=bins,\n",
    "            label=\"Correct passage\",\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    axs_first.axhline(0, color=\"k\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [\n",
    "    Plot.STEP_PENALTY_EP_LENGTH\n",
    "    ]:\n",
    "    selected_tag = Tag.CUMULATIVE_REWARD\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        step_penalty = summary_dict[id][\"file_contents\"][\"unity_config\"][\"stepPenalty\"]\n",
    "        run_label = f\"{step_penalty}\"\n",
    "\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=run_label\n",
    "            )\n",
    "\n",
    "    #axs_first.set_ylim(0, 200)\n",
    "    axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()   \n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result data\n",
    "To highlight the final performance of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_result_string(summary_dict: dict, id:int) -> str:\n",
    "\n",
    "    # Initialise the string.\n",
    "    s = \"\"\n",
    "\n",
    "    # Get y data.\n",
    "    y_collision_initial = get_y_data(summary_dict, id, Tag.COLLISION_INITIAL)\n",
    "    y_collision_stay = get_y_data(summary_dict, id, Tag.COLLISION_STAY)\n",
    "    y_cumulative_reward = get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD)\n",
    "    y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "    y_target_reached = get_y_data(summary_dict, id, Tag.TARGET_REACHED)\n",
    "\n",
    "    # Get x data.\n",
    "    x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "    # Get other data.\n",
    "    sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "\n",
    "    num_datapoints = 100\n",
    "\n",
    "    s += \"\\\\hline \\n\"\n",
    "    s += \"Parameter & Value & Performance Indicator & Value \\\\\\\\ \\n\"\n",
    "    s += \"\\\\hline \\n\"\n",
    "\n",
    "    final_cumulative_reward = np.mean(y_cumulative_reward[-num_datapoints:])\n",
    "    s+= f\"Range sensor count & {sensor_count} & Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\ \\n\"\n",
    "\n",
    "    final_cumulative_reward_std = np.std(y_cumulative_reward[-num_datapoints:])\n",
    "    s+= f\"Hidden layers & 1 & Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\ \\n\"\n",
    "\n",
    "    final_ep_length = np.mean(y_ep_length[-num_datapoints:])\n",
    "    s += f\"Hidden units & 512 & Episode length & {np.round(final_ep_length, 3)} \\\\\\\\ \\n\"\n",
    "    \n",
    "    target_rate = (np.mean(y_target_reached[-num_datapoints:]) * 100) / (10000 / np.mean(y_ep_length[-num_datapoints:]))\n",
    "    \n",
    "    #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "    steps_without_continued_col = 10000 - np.mean(y_collision_stay[-num_datapoints:])\n",
    "    final_collision_rate = (np.mean(y_collision_initial[-num_datapoints:]) / (steps_without_continued_col)) * 100\n",
    "    #print(100 -  / 10000 * 100)\n",
    "    s += f\"LSTM layer size & 64 & Target rate & {np.round(target_rate, 3)} \\% \\\\\\\\ \\n\"\n",
    "\n",
    "    s += f\"& & Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\ \\n\"\n",
    "    \n",
    "    performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "    performed_steps = \"{:.0e}\".format(performed_steps)\n",
    "    s += f\"& & Total performed steps & {performed_steps} \\\\\\\\ \\n\"\n",
    "    s += \"\\\\hline\"\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Env 1 data. 6459 is an applicable run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV1:\n",
    "    for id in selected_runs:\n",
    "        print(get_base_result_string(summary_dict, id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_quality(x, y):\n",
    "    return x / (x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env2_result_string(summary_dict: dict, id: int) -> str:\n",
    "    s = get_base_result_string(summary_dict, id)\n",
    "    s = \"\\\\hline\".join(s.split(\"\\\\hline\")[:-1])\n",
    "\n",
    "    # Get y data.\n",
    "    y_door_passage_incorrect = get_y_data(summary_dict, id, Tag.DOOR_BAD)\n",
    "    y_door_passage_correct = get_y_data(summary_dict, id, Tag.DOOR_GOOD)\n",
    "\n",
    "    # Get x data.\n",
    "    x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "    num_datapoints = 100\n",
    "\n",
    "    door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-num_datapoints:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-num_datapoints:])))\n",
    "    s += f\"& & Door passage quality & {np.round(door_passage_quality, 3)} \\\\\\\\ \\n\"\n",
    "    s += \"\\\\hline\"\n",
    "\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate environment 2 result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hline \n",
      "Parameter & Value & Performance Indicator & Value \\\\ \n",
      "\\hline \n",
      "Range sensor count & 32 & Cumulative reward & 0.822 \\\\ \n",
      "Hidden layers & 1 & Cumulative reward STD & 0.019 \\\\ \n",
      "Hidden units & 512 & Episode length & 63.684 \\\\ \n",
      "LSTM layer size & 64 & Target rate & 96.271 \\% \\\\ \n",
      "& & Collision rate & 0.085\\% \\\\ \n",
      "& & Total performed steps & 1e+08 \\\\ \n",
      "& & Door passage quality & 0.802 \\\\ \n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV2:\n",
    "    for id in selected_runs:\n",
    "        \"\"\"\n",
    "        y_collision_initial = get_y_data(summary_dict, id, Tag.COLLISION_INITIAL)\n",
    "        y_collision_stay = get_y_data(summary_dict, id, Tag.COLLISION_STAY)\n",
    "        y_cumulative_reward = get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD)\n",
    "        y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "        y_door_passage_incorrect = get_y_data(summary_dict, id, Tag.DOOR_BAD)\n",
    "        y_door_passage_correct = get_y_data(summary_dict, id, Tag.DOOR_GOOD)\n",
    "        \n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        num_datapoints = 100\n",
    "        \"\"\"\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-num_datapoints:])\n",
    "        #final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        #print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        #final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        #print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        #final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        #print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        #final_collision_rate = (np.mean(y_collision_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        #print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        #performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        #performed_steps = \"{:.0e}\".format(performed_steps)\n",
    "        #print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        num_datapoints = 100\n",
    "        print(\"\\\\hline\")\n",
    "        print(\"Parameter & Value & Performance Indicator & Value \\\\\\\\\")\n",
    "        print(\"\\\\hline\")\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        # Episode length.\n",
    "        final_cumulative_reward = np.mean(y_cumulative_reward[-num_datapoints:])\n",
    "        #final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Range sensor count & {sensor_count} & Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = np.std(y_cumulative_reward[-num_datapoints:])\n",
    "        #final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Hidden layers & 1 & Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-num_datapoints:])\n",
    "        print(f\"Hidden units & 512 & Episode length & {np.round(final_ep_length, 3)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        steps_without_continued_col = 10000 - np.mean(y_collision_stay[-num_datapoints:])\n",
    "        final_collision_rate = (np.mean(y_collision_initial[-num_datapoints:]) / (steps_without_continued_col)) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"LSTM layer size & 64 & Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        performed_steps = \"{:.0e}\".format(performed_steps)\n",
    "        print(f\"& & Total performed steps & {performed_steps} \\\\\\\\\")\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-num_datapoints:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-num_datapoints:])))\n",
    "        print(f\"& & Door passage quality & {np.round(door_passage_quality, 3)} \\\\\\\\\")\n",
    "        print(\"\\\\hline\")\n",
    "        \"\"\"\n",
    "        print(get_env2_result_string(summary_dict, id))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Latex table containing sensor study results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position(x, min_val, max_val):\n",
    "    # Calculate the midpoint\n",
    "    midpoint = (min_val + max_val) / 2.0\n",
    "    \n",
    "    # Calculate the position\n",
    "    position = 2 * (x - midpoint) / (max_val - min_val)\n",
    "    \n",
    "    return position\n",
    "\n",
    "def get_colour_string(value, data: dict, tag, low_is_good = False) -> str:\n",
    "    #tag = \"final_ep_length\"\n",
    "    #value = 99.36\n",
    "    #low_is_good = True\n",
    "\n",
    "    all_values = []\n",
    "    for id in data.keys():\n",
    "        all_values.append(data[id][tag])\n",
    "\n",
    "    \"\"\"\n",
    "    if low_is_good:\n",
    "        all_values_sorted = np.flip(np.sort(all_values))\n",
    "    else: # Normal case: High value equals good.\n",
    "        all_values_sorted = np.sort(all_values)\n",
    "\n",
    "    index = np.where(all_values_sorted == value)[0][0]\n",
    "\n",
    "    colours = [\n",
    "        \"{red!90}\",\n",
    "        \"{red!75}\",\n",
    "        \"{red!50}\",\n",
    "        \"{red!25}\",\n",
    "        \"{yellow!50}\",\n",
    "        \"{green!25}\",\n",
    "        \"{green!50}\",\n",
    "        \"{green!75}\",\n",
    "        \"{green!90}\",\n",
    "    ]\n",
    "\n",
    "    return f\"\\\\cellcolor{colours[index]}\"\n",
    "    \"\"\"\n",
    "\n",
    "    max = np.max(all_values)\n",
    "    min = np.min(all_values)\n",
    "\n",
    "    pos = calculate_position(value, min, max)\n",
    "\n",
    "    max_alpha_value = 80\n",
    "\n",
    "    if low_is_good:\n",
    "        if pos <= 0:\n",
    "            s = f\"{{green!{np.round(pos * -max_alpha_value, 2)}}}\"\n",
    "        else:\n",
    "            s = f\"{{red!{np.round(pos * max_alpha_value, 2)}}}\"\n",
    "    else:\n",
    "        if pos <= 0:\n",
    "            s = f\"{{red!{np.round(pos * -max_alpha_value, 2)}}}\"\n",
    "        else:\n",
    "            s = f\"{{green!{np.round(pos * max_alpha_value, 2)}}}\"\n",
    "\n",
    "    return f\"\\\\cellcolor{s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_SENSOR_STUDY:\n",
    "    run_data = {}\n",
    "\n",
    "    for id in selected_runs:\n",
    "        run_data[id] = {}\n",
    "        y_collision_initial = get_y_data(summary_dict, id, Tag.COLLISION_INITIAL)\n",
    "        y_collision_stay = get_y_data(summary_dict, id, Tag.COLLISION_STAY)\n",
    "        y_cumulative_reward = get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD)\n",
    "        y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "        y_door_passage_incorrect = get_y_data(summary_dict, id, Tag.DOOR_BAD)\n",
    "        y_door_passage_correct = get_y_data(summary_dict, id, Tag.DOOR_GOOD)\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "        num_datapoints = 100\n",
    "        \n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[id][\"final_mean_reward\"]\n",
    "        #print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"final_cumulative_reward\"] = np.round(final_cumulative_reward, 3)\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[id][\"final_std_reward\"]\n",
    "        #print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"final_cumulative_reward_std\"] = np.round(final_cumulative_reward_std, 3)\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        #print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "        run_data[id][\"final_ep_length\"] = np.round(np.round(final_ep_length, 2), 3)\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_collision_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        #print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "        run_data[id][\"final_collision_rate\"] = np.round(final_collision_rate, 3)\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        #print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "        run_data[id][\"performed_steps\"] = performed_steps\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-10:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-10:])))\n",
    "        #print(f\"Door passage quality $[-1, 1]$ & {np.round(door_passage_quality, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"door_passage_quality\"] = np.round(door_passage_quality, 3)\n",
    "        run_data[id][\"door_passage_incorrect\"] = -1 * np.mean(y_door_passage_incorrect[-10:])\n",
    "        run_data[id][\"door_passage_correct\"] = np.mean(y_door_passage_correct[-10:])\n",
    "        run_data[id][\"door_passage_total\"] = run_data[id][\"door_passage_incorrect\"] + run_data[id][\"door_passage_correct\"]\n",
    "        \n",
    "        run_data[id][\"sensor_count\"] = summary_dict[id][\"file_contents\"]['stats']['sensorCount']\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"& \\\\multicolumn{9}{c}{Sensor count} \\\\\\\\\")\n",
    "    \n",
    "    s = f\"Performance Indicator & \"\n",
    "    for id in selected_runs:\n",
    "        s+= f\"{run_data[id]['sensor_count']} & \"\n",
    "    s = s[:-2] + \"\\\\\\\\\"\n",
    "    print(s)\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    colour_cell = True\n",
    "    low_good = True\n",
    "    high_good = False\n",
    "\n",
    "    performance_indicators = [\n",
    "        (\"Sensors\", \"sensor_count\"),\n",
    "        (\"Cumulative reward\", \"final_cumulative_reward\", high_good),\n",
    "        (\"Cumulative reward STD\", \"final_cumulative_reward_std\", low_good),\n",
    "        (\"Episode length\", \"final_ep_length\", low_good),\n",
    "        (\"Collision rate\", \"final_collision_rate\", low_good),\n",
    "        (\"Door passage quality\",\"door_passage_quality\", high_good),\n",
    "        #(\"Incorrect Door passages\",\"door_passage_incorrect\"),\n",
    "        #(\"Correct Door passages\",\"door_passage_correct\")\n",
    "        (\"Door passage count\",\"door_passage_total\", high_good)\n",
    "    ]\n",
    "    for indicator in performance_indicators:\n",
    "        if indicator == (\"Sensors\", \"sensor_count\"):\n",
    "            continue\n",
    "        s = f\"{indicator[0]} & \"\n",
    "\n",
    "        for id in run_data.keys():\n",
    "            value = run_data[id][indicator[1]]\n",
    "            s += f\"{get_colour_string(value, run_data, indicator[1], low_is_good=indicator[2])}{value} & \"\n",
    "        s = s[:-2] + \"\\\\\\\\\"\n",
    "        print(s)\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 5\n",
    "min = 1\n",
    "middle = max - min\n",
    "value = 1\n",
    "\n",
    "calculate_position(value, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.0 * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b827e17ac296308a841e2dc6cc282cd5ca71c13fcd532c5acf2c55a086afa311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
