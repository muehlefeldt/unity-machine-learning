{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import scipy.constants as sc\n",
    "from scipy.signal import savgol_filter\n",
    "#from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "#plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    CUMULATIVE_REWARD = \"cumulative_rewards\"\n",
    "    EP_LENGTH = \"ep_length\"\n",
    "    DOOR_PASSAGE = \"passage\"\n",
    "    COLLISION_INITIAL = \"initial\"\n",
    "    COLLISION_STAY = \"stay\"\n",
    "    COLLISION_TOTAL = \"total\"\n",
    "    DOOR_GOOD = \"good_passage\"\n",
    "    DOOR_BAD = \"bad_passage\"\n",
    "\n",
    "class Plot(Enum):\n",
    "    COLLISION = 0\n",
    "    ENV1 = 1\n",
    "    ENV2 = 2\n",
    "    LINEPLOT = 3\n",
    "    RESULT_DATA_ENV1 = 4\n",
    "    RESULT_DATA_ENV2 = 5\n",
    "    DOOR = 6\n",
    "    DOOR_HIST = 7\n",
    "    SENSOR_STUDY_EP_LENGTH = 8\n",
    "    SENSOR_STUDY_REWARD = 9\n",
    "    SENSOR_STUDY_DOOR = 10\n",
    "    SENSOR_STUDY_INITIAL_COLLISION = 11\n",
    "    RESULT_DATA_SENSOR_STUDY = 12\n",
    "\n",
    "tag_colors = {\n",
    "    Tag.CUMULATIVE_REWARD: \"C1\",\n",
    "    Tag.EP_LENGTH: \"C2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from what run to get data. Also choose the Type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_runs = True\n",
    "selected_runs: list[int] = list(range(6466, 6474)) #[6072]\n",
    "plot_step = 5\n",
    "plot_all = True\n",
    "\n",
    "# Select the plot you want:\n",
    "selected_plot = Plot.RESULT_DATA_SENSOR_STUDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get y data fro the summary dict using the ID of the training run.\n",
    "The tag specifies the selected type of data.\"\"\"\n",
    "def get_y_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    try:\n",
    "        if tag in [Tag.CUMULATIVE_REWARD, Tag.EP_LENGTH]:\n",
    "            return summary_dict[id][\"env\"][tag.value]\n",
    "        if tag in [Tag.DOOR_PASSAGE, Tag.DOOR_BAD, Tag.DOOR_GOOD]:\n",
    "            return summary_dict[id][\"door\"][tag.value]\n",
    "        if tag in [Tag.COLLISION_STAY, Tag.COLLISION_INITIAL, Tag.COLLISION_STAY]:\n",
    "            return summary_dict[id][\"collision\"][tag.value]\n",
    "    except ValueError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get x data from the summary dict. Is based on the recorded step.\"\"\"\n",
    "def get_x_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    d = len(get_y_data(summary_dict, id, tag))\n",
    "    return summary_dict[id][\"steps\"][:d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter data to smooth plots.\"\"\"\n",
    "def filter_data(data):\n",
    "    try: \n",
    "        return savgol_filter(data, 15, 2)\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_arrays_to_same_length(a: np.ndarray, b:np.ndarray):\n",
    "    if len(a) != len(b):\n",
    "        if len(a) < len(b):\n",
    "            d = len(a)\n",
    "            b = b[:d]\n",
    "        else:\n",
    "            d = len(b)\n",
    "            a = a[:d]\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle with all the run data.\n",
    "summary_file_path = Path(\"C:/Users/max.muehlefeldt/Documents/GitHub/unity-machine-learning/python/basic_rl_env/summary_dict.pickle\").absolute()\n",
    "\n",
    "with open(summary_file_path, mode=\"rb\") as file:\n",
    "    summary_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "matplotlib.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [\n",
    "    Plot.SENSOR_STUDY_EP_LENGTH, \n",
    "    Plot.SENSOR_STUDY_REWARD,\n",
    "    Plot.SENSOR_STUDY_INITIAL_COLLISION\n",
    "    ]:\n",
    "    selected_tag = Tag.CUMULATIVE_REWARD\n",
    "    if selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_INITIAL_COLLISION:\n",
    "        selected_tag = Tag.COLLISION_INITIAL\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        \n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag]\n",
    "            label=run_label\n",
    "            )\n",
    "    \n",
    "    \n",
    "    if selected_plot is Plot.SENSOR_STUDY_REWARD:\n",
    "        axs_first.set_ylim(-2, 1.6)\n",
    "        axs_first.axhline(1.5, label=\"Reward limit = 1.5\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "        axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    elif selected_plot is Plot.SENSOR_STUDY_INITIAL_COLLISION:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Initial Collisions\")\n",
    "        \n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()   \n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.COLLISION:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        #y = savgol_filter( get_y_data(summary_dict, id, selected_tag), 15, 2)\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        #axs_first.plot(\n",
    "        #    x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag]\n",
    "        #    label=run_label\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        y_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "\n",
    "        if len(y_initial) != len(y_stay):\n",
    "            if len(y_initial) < len(y_stay):\n",
    "                d = len(y_initial)\n",
    "                y_stay = y_stay[:d]\n",
    "                x = x[:d]\n",
    "            else:\n",
    "                d = len(y_stay)\n",
    "                y_initial = y_initial[:d]\n",
    "                x = x[:d]\n",
    "        \n",
    "        y = np.vstack([y_initial, y_stay])\n",
    "\n",
    "        \n",
    "\n",
    "        axs_first.stackplot(\n",
    "            x, y,\n",
    "            labels=[\"Initial contact\", \"Continued contact\"],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        axs_first.set_ylim(0, 200)\n",
    "    #if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "    #    #axs_first.set_ylim(-2, 1.5)\n",
    "    #    axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "    #    axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Collisions\")\n",
    "\n",
    "    #elif selected_tag is Tag.EP_LENGTH:\n",
    "    #    axs_first.set_ylim(0, 200)\n",
    "    #    axs_first.set_ylabel(\"Length\")\n",
    "    #    axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    #elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "    #    axs_first.set_ylim(-1, 1)\n",
    "    #    axs_first.set_ylabel(\"Passage Quality\")\n",
    "    #    axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "        axs_first.set_xlabel(\"Step\")\n",
    "        \n",
    "        axs_first.legend(title=f\"Run {id}\")\n",
    "        axs_first.grid()   \n",
    "\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        plt.style.use(\"default\")\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "        fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [Plot.ENV1, Plot.ENV2]:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "\n",
    "        run_label = f\"Cumulutive reward\"\n",
    "        selected_tag = Tag.CUMULATIVE_REWARD\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Cumulative reward\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_first.set_ylim(\n",
    "            -2, \n",
    "            1.1 if selected_plot is Plot.ENV1 else 1.6\n",
    "        )\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        reward_label = \"Reward limit = 1.0\" if selected_plot is Plot.ENV1 else  \"Reward limit = 1.5\"\n",
    "        axs_first.axhline(\n",
    "            1.0 if selected_plot is Plot.ENV1 else 1.5,\n",
    "            label=reward_label, color=\"C3\", linestyle=\"dashed\"\n",
    "        )\n",
    "\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        axs_second = axs_first.twinx()\n",
    "        axs_second.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Episode length\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_second.set_ylim(0, 200)\n",
    "        axs_second.set_ylabel(\"Episode Length\")\n",
    "        #fig.suptitle(\"Cumulative reward and episode length\")\n",
    "    \n",
    "    \"\"\"if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "        #axs_first.set_ylim(-2, 1.5)\n",
    "        axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_tag is Tag.EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\"\"\"\n",
    "    axs_first.set_title(\"Cumulative reward and episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_second.legend()\n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_second.grid()\n",
    "    #axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        y = -1 * y\n",
    "\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag],\n",
    "            label=\"Incorrect passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "       \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=\"Correct passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    \n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR_HIST:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_bad = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #y = -1 * y\n",
    "\n",
    "\n",
    "        #axs_first.hist(\n",
    "        #    #x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag],\n",
    "        #    label=\"Incorrect passage\",\n",
    "        #    linewidth=2.0,\n",
    "        #    )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_good = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "\n",
    "        y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "\n",
    "        # Use a constant bin width to make the two histograms easier to compare visually\n",
    "        #bin_width = 1\n",
    "        #bins = np.arange(np.min([y_bad, y_good]),\n",
    "                            #np.max([y_bad, y_good]) + bin_width, bin_width)\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_bad,\n",
    "            weights=-np.ones_like(y_bad),\n",
    "            #bins=bins,\n",
    "            label=\"Inorrect passage\",\n",
    "            )\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_good,\n",
    "            #bins=bins,\n",
    "            label=\"Correct passage\",\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    axs_first.axhline(0, color=\"k\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result data\n",
    "To highlight the final performance of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV1:\n",
    "    for id in selected_runs:\n",
    "        y_collision_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_collision_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "        y_cumulative_reward = filter_data(get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD))\n",
    "        y_ep_length = filter_data(get_y_data(summary_dict, id, Tag.EP_LENGTH))\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        print(f\"Episode length & {np.round(final_ep_length, 3)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_collision_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        performed_steps = \"{:.0e}\".format(performed_steps)\n",
    "        print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_quality(x, y):\n",
    "    return x / (x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate environment 2 result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV2:\n",
    "    for id in selected_runs:\n",
    "        y_collision_initial = get_y_data(summary_dict, id, Tag.COLLISION_INITIAL)\n",
    "        y_collision_stay = get_y_data(summary_dict, id, Tag.COLLISION_STAY)\n",
    "        y_cumulative_reward = get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD)\n",
    "        y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "        y_door_passage_incorrect = get_y_data(summary_dict, id, Tag.DOOR_BAD)\n",
    "        y_door_passage_correct = get_y_data(summary_dict, id, Tag.DOOR_GOOD)\n",
    "        \n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_collision_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        performed_steps = \"{:.0e}\".format(performed_steps)\n",
    "        print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-10:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-10:])))\n",
    "        print(f\"Door passage quality & {np.round(door_passage_quality, 3)} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Latex table containing sensor study results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position(x, min_val, max_val):\n",
    "    # Calculate the midpoint\n",
    "    midpoint = (min_val + max_val) / 2.0\n",
    "    \n",
    "    # Calculate the position\n",
    "    position = 2 * (x - midpoint) / (max_val - min_val)\n",
    "    \n",
    "    return position\n",
    "\n",
    "def get_colour_string(value, data: dict, tag, low_is_good = False) -> str:\n",
    "    #tag = \"final_ep_length\"\n",
    "    #value = 99.36\n",
    "    #low_is_good = True\n",
    "\n",
    "    all_values = []\n",
    "    for id in data.keys():\n",
    "        all_values.append(data[id][tag])\n",
    "\n",
    "    \"\"\"\n",
    "    if low_is_good:\n",
    "        all_values_sorted = np.flip(np.sort(all_values))\n",
    "    else: # Normal case: High value equals good.\n",
    "        all_values_sorted = np.sort(all_values)\n",
    "\n",
    "    index = np.where(all_values_sorted == value)[0][0]\n",
    "\n",
    "    colours = [\n",
    "        \"{red!90}\",\n",
    "        \"{red!75}\",\n",
    "        \"{red!50}\",\n",
    "        \"{red!25}\",\n",
    "        \"{yellow!50}\",\n",
    "        \"{green!25}\",\n",
    "        \"{green!50}\",\n",
    "        \"{green!75}\",\n",
    "        \"{green!90}\",\n",
    "    ]\n",
    "\n",
    "    return f\"\\\\cellcolor{colours[index]}\"\n",
    "    \"\"\"\n",
    "\n",
    "    max = np.max(all_values)\n",
    "    min = np.min(all_values)\n",
    "\n",
    "    pos = calculate_position(value, min, max)\n",
    "\n",
    "    if low_is_good:\n",
    "        if pos <= 0:\n",
    "            s = f\"{{green!{pos * -100}}}\"\n",
    "        else:\n",
    "            s = f\"{{red!{pos * 100}}}\"\n",
    "    else:\n",
    "        if pos <= 0:\n",
    "            s = f\"{{red!{pos * -100}}}\"\n",
    "        else:\n",
    "            s = f\"{{green!{pos * 100}}}\"\n",
    "\n",
    "    return f\"\\\\cellcolor{s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hline\n",
      "Indicator / Sensors & 1 & 2 & 4 & 8 & 10 & 16 & 32 & 64 \\\\\n",
      "\\hline\n",
      "Cumulative reward & \\cellcolor{red!99.17808219178083}-0.106 & \\cellcolor{red!100.0}-0.109 & \\cellcolor{red!86.02739726027397}-0.058 & \\cellcolor{red!59.178082191780824}0.04 & \\cellcolor{green!63.56164383561643}0.488 & \\cellcolor{green!80.00000000000001}0.548 & \\cellcolor{green!93.97260273972601}0.599 & \\cellcolor{green!100.0}0.621 \\\\\n",
      "Cumulative reward STD & \\cellcolor{green!17.64705882352943}0.089 & \\cellcolor{green!52.94117647058826}0.071 & \\cellcolor{red!7.843137254901941}0.102 & \\cellcolor{red!99.99999999999999}0.149 & \\cellcolor{green!84.31372549019609}0.055 & \\cellcolor{green!74.50980392156865}0.06 & \\cellcolor{green!100.00000000000003}0.047 & \\cellcolor{green!98.03921568627452}0.048 \\\\\n",
      "Episode length & \\cellcolor{red!99.99999999999997}159.31 & \\cellcolor{red!99.93841416474206}159.29 & \\cellcolor{red!65.69668976135482}148.17 & \\cellcolor{red!15.812163202463406}131.97 & \\cellcolor{green!49.65357967667441}110.71 & \\cellcolor{green!80.5696689761355}100.67 & \\cellcolor{green!84.60354118552735}99.36 & \\cellcolor{green!100.00000000000003}94.36 \\\\\n",
      "Collision rate & \\cellcolor{red!76.47058823529413}0.109 & \\cellcolor{green!21.568627450980355}0.084 & \\cellcolor{red!100.00000000000003}0.115 & \\cellcolor{green!60.78431372549019}0.074 & \\cellcolor{green!80.39215686274505}0.069 & \\cellcolor{green!25.49019607843134}0.083 & \\cellcolor{green!49.019607843137244}0.077 & \\cellcolor{green!99.99999999999997}0.064 \\\\\n",
      "Door passage quality & \\cellcolor{red!77.48344370860926}0.58 & \\cellcolor{red!50.993377483443666}0.6 & \\cellcolor{green!21.85430463576168}0.655 & \\cellcolor{red!100.0}0.563 & \\cellcolor{green!50.99337748344382}0.677 & \\cellcolor{green!100.0}0.714 & \\cellcolor{green!97.35099337748345}0.712 & \\cellcolor{green!57.61589403973522}0.682 \\\\\n",
      "Door Passage Count & \\cellcolor{red!66.29213483146067}20.0 & \\cellcolor{red!100.0}0.5 & \\cellcolor{red!95.85133967156439}2.9 & \\cellcolor{red!37.94295592048401}36.4 & \\cellcolor{green!80.98530682800346}105.2 & \\cellcolor{green!73.89801210025928}101.1 & \\cellcolor{green!83.05963699222126}106.4 & \\cellcolor{green!100.0}116.2 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_SENSOR_STUDY:\n",
    "    run_data = {}\n",
    "\n",
    "    for id in selected_runs:\n",
    "        run_data[id] = {}\n",
    "        y_collision_initial = get_y_data(summary_dict, id, Tag.COLLISION_INITIAL)\n",
    "        y_collision_stay = get_y_data(summary_dict, id, Tag.COLLISION_STAY)\n",
    "        y_cumulative_reward = get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD)\n",
    "        y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "        y_door_passage_incorrect = get_y_data(summary_dict, id, Tag.DOOR_BAD)\n",
    "        y_door_passage_correct = get_y_data(summary_dict, id, Tag.DOOR_GOOD)\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[id][\"final_mean_reward\"]\n",
    "        #print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"final_cumulative_reward\"] = np.round(final_cumulative_reward, 3)\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[id][\"final_std_reward\"]\n",
    "        #print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"final_cumulative_reward_std\"] = np.round(final_cumulative_reward_std, 3)\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        #print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "        run_data[id][\"final_ep_length\"] = np.round(np.round(final_ep_length, 2), 3)\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_collision_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        #print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "        run_data[id][\"final_collision_rate\"] = np.round(final_collision_rate, 3)\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        #print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "        run_data[id][\"performed_steps\"] = performed_steps\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-10:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-10:])))\n",
    "        #print(f\"Door passage quality $[-1, 1]$ & {np.round(door_passage_quality, 3)} \\\\\\\\\")\n",
    "        run_data[id][\"door_passage_quality\"] = np.round(door_passage_quality, 3)\n",
    "        run_data[id][\"door_passage_incorrect\"] = -1 * np.mean(y_door_passage_incorrect[-10:])\n",
    "        run_data[id][\"door_passage_correct\"] = np.mean(y_door_passage_correct[-10:])\n",
    "        run_data[id][\"door_passage_total\"] = run_data[id][\"door_passage_incorrect\"] + run_data[id][\"door_passage_correct\"]\n",
    "        \n",
    "        run_data[id][\"sensor_count\"] = summary_dict[id][\"file_contents\"]['stats']['sensorCount']\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    s = f\"Indicator / Sensors & \"\n",
    "    for id in selected_runs:\n",
    "        s+= f\"{run_data[id]['sensor_count']} & \"\n",
    "    s = s[:-2] + \"\\\\\\\\\"\n",
    "    print(s)\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    colour_cell = True\n",
    "    low_good = True\n",
    "    high_good = False\n",
    "\n",
    "    performance_indicators = [\n",
    "        (\"Sensors\", \"sensor_count\"),\n",
    "        (\"Cumulative reward\", \"final_cumulative_reward\", high_good),\n",
    "        (\"Cumulative reward STD\", \"final_cumulative_reward_std\", low_good),\n",
    "        (\"Episode length\", \"final_ep_length\", low_good),\n",
    "        (\"Collision rate\", \"final_collision_rate\", low_good),\n",
    "        (\"Door passage quality\",\"door_passage_quality\", high_good),\n",
    "        #(\"Incorrect Door passages\",\"door_passage_incorrect\"),\n",
    "        #(\"Correct Door passages\",\"door_passage_correct\")\n",
    "        (\"Door Passage Count\",\"door_passage_total\", high_good)\n",
    "    ]\n",
    "    for indicator in performance_indicators:\n",
    "        if indicator == (\"Sensors\", \"sensor_count\"):\n",
    "            continue\n",
    "        s = f\"{indicator[0]} & \"\n",
    "\n",
    "        for id in run_data.keys():\n",
    "            value = run_data[id][indicator[1]]\n",
    "            s += f\"{get_colour_string(value, run_data, indicator[1], low_is_good=indicator[2])}{value} & \"\n",
    "        s = s[:-2] + \"\\\\\\\\\"\n",
    "        print(s)\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 5\n",
    "min = 1\n",
    "middle = max - min\n",
    "value = 1\n",
    "\n",
    "calculate_position(value, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 1154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.0 * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b827e17ac296308a841e2dc6cc282cd5ca71c13fcd532c5acf2c55a086afa311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
