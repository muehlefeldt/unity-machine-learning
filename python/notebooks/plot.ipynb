{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import scipy.constants as sc\n",
    "from scipy.signal import savgol_filter\n",
    "#from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "#plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    CUMULATIVE_REWARD = \"cumulative_rewards\"\n",
    "    EP_LENGTH = \"ep_length\"\n",
    "    DOOR_PASSAGE = \"passage\"\n",
    "    COLLISION_INITIAL = \"initial\"\n",
    "    COLLISION_STAY = \"stay\"\n",
    "    COLLISION_TOTAL = \"total\"\n",
    "    DOOR_GOOD = \"good_passage\"\n",
    "    DOOR_BAD = \"bad_passage\"\n",
    "    TARGET_REACHED = \"target_reached\"\n",
    "\n",
    "class Plot(Enum):\n",
    "    COLLISION = 0\n",
    "    ENV1 = 1\n",
    "    ENV2 = 2\n",
    "    LINEPLOT = 3\n",
    "    RESULT_DATA_ENV1 = 4\n",
    "    RESULT_DATA_ENV2 = 5\n",
    "    DOOR = 6\n",
    "    DOOR_HIST = 7\n",
    "    SENSOR_STUDY_EP_LENGTH = 8\n",
    "    SENSOR_STUDY_REWARD = 9\n",
    "    SENSOR_STUDY_DOOR_GOOD = 10,\n",
    "    SENSOR_STUDY_DOOR_BAD = 11,\n",
    "    SENSOR_STUDY_INITIAL_COLLISION = 12\n",
    "    SENSOR_STUDY_CONTINUED_COLLISION = 13\n",
    "    SENSOR_STUDY_DOOR_QUALITY = 14\n",
    "    RESULT_DATA_SENSOR_STUDY = 15\n",
    "    STEP_PENALTY_EP_LENGTH = 16\n",
    "\n",
    "tag_colors = {\n",
    "    Tag.CUMULATIVE_REWARD: \"C1\",\n",
    "    Tag.EP_LENGTH: \"C2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a colormap to ensure constant color choice throughout the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAGHRFWHRUaXRsZQB0ZXJyYWluX3IgY29sb3JtYXAofhsCAAAAHnRFWHREZXNjcmlwdGlvbgB0ZXJyYWluX3IgY29sb3JtYXBq5f2JAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My44LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmefc/hPAAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ7HVZ2gAAAHeSURBVHic7dbLboMwFEXRi1P1//+3CXSAqZSHlVerpDprTSwTsHEGaE/LsixVVfPhUEfjvM3nk/n2+1yXn5vfbJ1X7//e51iWdWy7qaqqpraOuz5vfd6G83b8fB93V5+7PP95frD+tXXO3v/Oc5zvf9864/1vO8fz+7e7zr+sQ+3Xz0Dt62Q+Gv/ovq8+Pwzu++r3DX/fnh/se7b+i887eq/Tcwx/H/wvt65X80cfP//5uJ5j6t+51r972/zR61O/3k7v79/PaX7y+i+956PX168FABBFAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIG+AYFFzI3iWqoCAAAAAElFTkSuQmCC",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>terrain_r</strong> </div><div class=\"cmap\"><img alt=\"terrain_r colormap\" title=\"terrain_r\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAGHRFWHRUaXRsZQB0ZXJyYWluX3IgY29sb3JtYXAofhsCAAAAHnRFWHREZXNjcmlwdGlvbgB0ZXJyYWluX3IgY29sb3JtYXBq5f2JAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My44LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmefc/hPAAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ7HVZ2gAAAHeSURBVHic7dbLboMwFEXRi1P1//+3CXSAqZSHlVerpDprTSwTsHEGaE/LsixVVfPhUEfjvM3nk/n2+1yXn5vfbJ1X7//e51iWdWy7qaqqpraOuz5vfd6G83b8fB93V5+7PP95frD+tXXO3v/Oc5zvf9864/1vO8fz+7e7zr+sQ+3Xz0Dt62Q+Gv/ovq8+Pwzu++r3DX/fnh/se7b+i887eq/Tcwx/H/wvt65X80cfP//5uJ5j6t+51r972/zR61O/3k7v79/PaX7y+i+956PX168FABBFAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIG+AYFFzI3iWqoCAAAAAElFTkSuQmCC\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#ffffffff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #ffffffff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#333399ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #333399ff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.LinearSegmentedColormap at 0x2189e919af0>"
      ]
     },
     "execution_count": 3394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap = plt.get_cmap('terrain').reversed()  # Choose a colormap\n",
    "#num_runs = len(selected_runs)\n",
    "\n",
    "colors = [cmap(i / 11) for i in range(2, 12, 1)]\n",
    "colors_sensor = {\n",
    "    1: colors[0],\n",
    "    2: colors[1],\n",
    "    3: colors[2],\n",
    "    4: colors[3],\n",
    "    8: colors[4],\n",
    "    16: colors[5],\n",
    "    32: colors[6],\n",
    "    64: colors[7],\n",
    "    128: colors[8],\n",
    "}\n",
    "#cmap.reversed()\n",
    "def get_color_by_sensor_count(sen_count: int):\n",
    "    return colors_sensor[sen_count]\n",
    "\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\definecolor{1Color}{rgb}{0.64, 0.54, 0.52}\n",
      "\\definecolor{2Color}{rgb}{0.54, 0.41, 0.35}\n",
      "\\definecolor{3Color}{rgb}{0.73, 0.65, 0.45}\n",
      "\\definecolor{4Color}{rgb}{0.91, 0.88, 0.55}\n",
      "\\definecolor{8Color}{rgb}{0.82, 0.96, 0.56}\n",
      "\\definecolor{16Color}{rgb}{0.46, 0.89, 0.49}\n",
      "\\definecolor{32Color}{rgb}{0.08, 0.82, 0.42}\n",
      "\\definecolor{64Color}{rgb}{0.0, 0.66, 0.82}\n",
      "\\definecolor{128Color}{rgb}{0.08, 0.44, 0.84}\n"
     ]
    }
   ],
   "source": [
    "# Print Latex code to define colors.\n",
    "\n",
    "for key in colors_sensor.keys():\n",
    "    rgb_color = tuple(np.round(x, 2) for x in colors_sensor[key][:3])\n",
    "    color_name = f\"{key}Color\"\n",
    "    \n",
    "    print(f\"\\\\definecolor{{{color_name}}}{{rgb}}{{{rgb_color[0]}, {rgb_color[1]}, {rgb_color[2]}}}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-selected run ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1_id = [6459]\n",
    "env2_id = [6465]\n",
    "sensor_study_ids = [\n",
    "    6466, # 1 \n",
    "    6467, # 2\n",
    "    6480, # 3\n",
    "    6468, # 4\n",
    "    6469, # 8\n",
    "    #6470, # 10\n",
    "    6471, # 16\n",
    "    6472, # 32\n",
    "    6473, # 64\n",
    "    6474, # 128\n",
    "]\n",
    "\n",
    "len(sensor_study_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from what run to get data. Also choose the Type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the plot type you want.\n",
    "selected_plot = Plot.RESULT_DATA_SENSOR_STUDY\n",
    "\n",
    "# Select here the runs to be ploted.\n",
    "selected_runs: list[int] = sensor_study_ids\n",
    "\n",
    "# Legacy options. No longer in use?\n",
    "plot_step = 5\n",
    "plot_all = True\n",
    "multiple_runs = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3398,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get y data fro the summary dict using the ID of the training run.\n",
    "The tag specifies the selected type of data.\"\"\"\n",
    "def get_y_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    try:\n",
    "        if tag in [Tag.CUMULATIVE_REWARD, Tag.EP_LENGTH, Tag.TARGET_REACHED]:\n",
    "            return summary_dict[id][\"env\"][tag.value]\n",
    "        if tag in [Tag.DOOR_PASSAGE, Tag.DOOR_BAD, Tag.DOOR_GOOD]:\n",
    "            return summary_dict[id][\"door\"][tag.value]\n",
    "        if tag in [Tag.COLLISION_STAY, Tag.COLLISION_INITIAL, Tag.COLLISION_STAY]:\n",
    "            return summary_dict[id][\"collision\"][tag.value]\n",
    "    except ValueError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3399,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get x data from the summary dict. Is based on the recorded step.\"\"\"\n",
    "def get_x_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    d = len(get_y_data(summary_dict, id, tag))\n",
    "    return summary_dict[id][\"steps\"][:d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3400,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter data to smooth plots.\"\"\"\n",
    "def filter_data(data):\n",
    "    try: \n",
    "        return savgol_filter(data, 15, 2)\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_arrays_to_same_length(a: np.ndarray, b:np.ndarray):\n",
    "    if len(a) != len(b):\n",
    "        if len(a) < len(b):\n",
    "            d = len(a)\n",
    "            b = b[:d]\n",
    "        else:\n",
    "            d = len(b)\n",
    "            a = a[:d]\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_quality(good, bad):\n",
    "    x = good\n",
    "    y = bad\n",
    "    try: \n",
    "        result = x / (x + y)\n",
    "    except RuntimeWarning:\n",
    "        result = 0.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the summary pickle file. Needs to be created beforehand by runnning `results_summary_pickle.py`. Otherwise it takes ages to get the plot data. Trust me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle with all the run data.\n",
    "summary_file_path = Path(\"C:/Users/max.muehlefeldt/Documents/GitHub/unity-machine-learning/python/basic_rl_env/summary_dict.pickle\").absolute()\n",
    "\n",
    "with open(summary_file_path, mode=\"rb\") as file:\n",
    "    summary_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "matplotlib.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3405,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [\n",
    "    Plot.SENSOR_STUDY_EP_LENGTH, \n",
    "    Plot.SENSOR_STUDY_REWARD,\n",
    "    Plot.SENSOR_STUDY_INITIAL_COLLISION,\n",
    "    Plot.SENSOR_STUDY_CONTINUED_COLLISION,\n",
    "    Plot.SENSOR_STUDY_DOOR_GOOD,\n",
    "    Plot.SENSOR_STUDY_DOOR_BAD,\n",
    "    Plot.SENSOR_STUDY_DOOR_QUALITY\n",
    "    ]:\n",
    "    selected_tag = Tag.CUMULATIVE_REWARD\n",
    "    if selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_INITIAL_COLLISION:\n",
    "        selected_tag = Tag.COLLISION_INITIAL\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_CONTINUED_COLLISION:\n",
    "        selected_tag = Tag.COLLISION_STAY\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_GOOD:\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_BAD:\n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_QUALITY:\n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    #cmap = plt.get_cmap('terrain')  # Choose a colormap\n",
    "    #num_runs = len(selected_runs)\n",
    "    #colors = [cmap(i / num_runs) for i in range(num_runs)]  # Generate colors from the colormap\n",
    "\n",
    "    for idx, id in enumerate(selected_runs):\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        \n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        \n",
    "        if selected_plot is Plot.SENSOR_STUDY_DOOR_QUALITY:\n",
    "            selected_tag = Tag.DOOR_BAD\n",
    "            y_bad = -1 * np.array(get_y_data(summary_dict, id, selected_tag))\n",
    "            selected_tag = Tag.DOOR_GOOD\n",
    "            y_good = np.array(get_y_data(summary_dict, id, selected_tag))\n",
    "            y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "            #y = []\n",
    "            #for index in range(len(y_good)):\n",
    "                #y.append(get_passage_quality(y_good[index], y_bad[index]))\n",
    "            y = get_passage_quality(y_good, y_bad)\n",
    "        elif selected_plot is Plot.SENSOR_STUDY_CONTINUED_COLLISION:\n",
    "            y = filter_data(filter_data(filter_data(y)))\n",
    "        elif selected_plot is Plot.SENSOR_STUDY_DOOR_BAD:\n",
    "            y = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        else:\n",
    "            y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        #plt.set_cmap(\"Dark2\")\n",
    "\n",
    "\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=get_color_by_sensor_count(sensor_count),  # Apply color from colormap\n",
    "            label=run_label\n",
    "            )\n",
    "    \n",
    "    \n",
    "    if selected_plot is Plot.SENSOR_STUDY_REWARD:\n",
    "        axs_first.set_ylim(-2, 1.4)\n",
    "        axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        #axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "        axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    elif selected_plot is Plot.SENSOR_STUDY_INITIAL_COLLISION:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Initial Collisions\")\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_CONTINUED_COLLISION:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Continued Collisions\")\n",
    "    \n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_GOOD:\n",
    "        axs_first.set_title(\"Correct Door Passages\")\n",
    "        axs_first.set_ylim(0, 95)\n",
    "\n",
    "    elif selected_plot is Plot.SENSOR_STUDY_DOOR_BAD:\n",
    "        axs_first.set_title(\"Incorrect Door Passages\")\n",
    "        axs_first.set_ylim(0, 95)\n",
    "        \n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    if selected_plot is Plot.SENSOR_STUDY_REWARD:\n",
    "        axs_first.legend(ncol=2)\n",
    "    else:\n",
    "        axs_first.legend()\n",
    "    axs_first.grid()\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    \n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sameRoom': 62061,\n",
       " 'agentInRoomID0': 62095,\n",
       " 'agentInRoomID1': 61705,\n",
       " 'targetInRoomID0': 61972,\n",
       " 'targetInRoomID1': 61828,\n",
       " 'episodeCount': 123800,\n",
       " 'sensorCount': 3,\n",
       " 'runId': 6480,\n",
       " 'targetRoomIndex0': 61972,\n",
       " 'targetRoomIndex1': 61828}"
      ]
     },
     "execution_count": 3406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dict[6480][\"file_contents\"][\"stats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3407,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.COLLISION:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        #y = savgol_filter( get_y_data(summary_dict, id, selected_tag), 15, 2)\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        #axs_first.plot(\n",
    "        #    x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag]\n",
    "        #    label=run_label\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        y_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "\n",
    "        if len(y_initial) != len(y_stay):\n",
    "            if len(y_initial) < len(y_stay):\n",
    "                d = len(y_initial)\n",
    "                y_stay = y_stay[:d]\n",
    "                x = x[:d]\n",
    "            else:\n",
    "                d = len(y_stay)\n",
    "                y_initial = y_initial[:d]\n",
    "                x = x[:d]\n",
    "        \n",
    "        y = np.vstack([y_initial, y_stay])\n",
    "\n",
    "        \n",
    "\n",
    "        axs_first.stackplot(\n",
    "            x, y,\n",
    "            labels=[\"Initial contact\", \"Continued contact\"],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        axs_first.set_ylim(0, 200)\n",
    "    #if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "    #    #axs_first.set_ylim(-2, 1.5)\n",
    "    #    axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "    #    axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Collisions\")\n",
    "\n",
    "    #elif selected_tag is Tag.EP_LENGTH:\n",
    "    #    axs_first.set_ylim(0, 200)\n",
    "    #    axs_first.set_ylabel(\"Length\")\n",
    "    #    axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    #elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "    #    axs_first.set_ylim(-1, 1)\n",
    "    #    axs_first.set_ylabel(\"Passage Quality\")\n",
    "    #    axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "        axs_first.set_xlabel(\"Step\")\n",
    "        \n",
    "        axs_first.legend(title=f\"Run {id}\")\n",
    "        axs_first.grid()   \n",
    "\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        plt.style.use(\"default\")\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "        fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3408,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [Plot.ENV1, Plot.ENV2]:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "\n",
    "        run_label = f\"Cumulutive reward\"\n",
    "        selected_tag = Tag.CUMULATIVE_REWARD\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Cumulative reward\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_first.set_ylim(\n",
    "            -2, \n",
    "            1.1 if selected_plot is Plot.ENV1 else 1.6\n",
    "        )\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        reward_label = \"Reward limit = 1.0\" if selected_plot is Plot.ENV1 else  \"Reward limit = 1.5\"\n",
    "        axs_first.axhline(\n",
    "            1.0 if selected_plot is Plot.ENV1 else 1.5,\n",
    "            label=reward_label, color=\"C3\", linestyle=\"dashed\"\n",
    "        )\n",
    "\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        axs_second = axs_first.twinx()\n",
    "        axs_second.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Episode length\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_second.set_ylim(0, 200)\n",
    "        axs_second.set_ylabel(\"Episode Length\")\n",
    "        #fig.suptitle(\"Cumulative reward and episode length\")\n",
    "    \n",
    "    \"\"\"if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "        #axs_first.set_ylim(-2, 1.5)\n",
    "        axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_tag is Tag.EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\"\"\"\n",
    "    axs_first.set_title(\"Cumulative reward and episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_second.legend()\n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_second.grid()\n",
    "    #axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3409,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        y = -1 * y\n",
    "\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag],\n",
    "            label=\"Incorrect passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "       \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=\"Correct passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    \n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3410,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR_HIST:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_bad = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #y = -1 * y\n",
    "\n",
    "\n",
    "        #axs_first.hist(\n",
    "        #    #x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag],\n",
    "        #    label=\"Incorrect passage\",\n",
    "        #    linewidth=2.0,\n",
    "        #    )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_good = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "\n",
    "        y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "\n",
    "        # Use a constant bin width to make the two histograms easier to compare visually\n",
    "        #bin_width = 1\n",
    "        #bins = np.arange(np.min([y_bad, y_good]),\n",
    "                            #np.max([y_bad, y_good]) + bin_width, bin_width)\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_bad,\n",
    "            weights=-np.ones_like(y_bad),\n",
    "            #bins=bins,\n",
    "            label=\"Inorrect passage\",\n",
    "            )\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_good,\n",
    "            #bins=bins,\n",
    "            label=\"Correct passage\",\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    axs_first.axhline(0, color=\"k\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3411,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [\n",
    "    Plot.STEP_PENALTY_EP_LENGTH\n",
    "    ]:\n",
    "    selected_tag = Tag.CUMULATIVE_REWARD\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        step_penalty = summary_dict[id][\"file_contents\"][\"unity_config\"][\"stepPenalty\"]\n",
    "        run_label = f\"{step_penalty}\"\n",
    "\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=run_label\n",
    "            )\n",
    "\n",
    "    #axs_first.set_ylim(0, 200)\n",
    "    axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()   \n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result data\n",
    "To highlight the final performance of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_result_string_data(summary_dict: dict, id:int) -> tuple[str, dict]:\n",
    "\n",
    "    # Initialise the string and data dict.\n",
    "    s = \"\"\n",
    "    data = {}\n",
    "\n",
    "    # Get y data.\n",
    "    y_collision_initial = get_y_data(summary_dict, id, Tag.COLLISION_INITIAL)\n",
    "    y_collision_stay = get_y_data(summary_dict, id, Tag.COLLISION_STAY)\n",
    "    y_cumulative_reward = get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD)\n",
    "    y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "    y_target_reached = get_y_data(summary_dict, id, Tag.TARGET_REACHED)\n",
    "\n",
    "    # Get x data.\n",
    "    x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "    # Get other data.\n",
    "    data[\"sensor_count\"] = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "\n",
    "    data[\"num_datapoints\"] = 100\n",
    "    num_datapoints = 100\n",
    "\n",
    "    s += \"\\\\hline \\n\"\n",
    "    s += \"Parameter & Value & Performance Indicator & Value \\\\\\\\ \\n\"\n",
    "    s += \"\\\\hline \\n\"\n",
    "\n",
    "    data[\"final_cumulative_reward\"] = np.mean(y_cumulative_reward[-num_datapoints:])\n",
    "    s+= f\"Range sensor count & {data['sensor_count']} & Cumulative reward & {np.round(data['final_cumulative_reward'], 2)} \\\\\\\\ \\n\"\n",
    "\n",
    "    data[\"final_cumulative_reward_std\"] = np.std(y_cumulative_reward[-num_datapoints:])\n",
    "    s+= f\"Hidden layers & 1 & Cumulative reward STD & {np.round(data['final_cumulative_reward_std'], 2)} \\\\\\\\ \\n\"\n",
    "\n",
    "    data[\"final_ep_length\"] = np.mean(y_ep_length[-num_datapoints:])\n",
    "    s += f\"Hidden units & 512 & Episode length & {np.round(data['final_ep_length'], 2)} \\\\\\\\ \\n\"\n",
    "    \n",
    "    data[\"target_rate\"] = (np.mean(y_target_reached[-num_datapoints:]) * 100) / (10000 / np.mean(y_ep_length[-num_datapoints:]))\n",
    "    \n",
    "    #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "    steps_without_continued_col = 10000 - np.mean(y_collision_stay[-num_datapoints:])\n",
    "    data[\"final_collision_rate\"] = (np.mean(y_collision_initial[-num_datapoints:]) / (steps_without_continued_col)) * 100\n",
    "    #print(100 -  / 10000 * 100)\n",
    "    s += f\"LSTM layer size & 64 & Target rate [\\\\%] & {np.round(data['target_rate'], 2)} \\\\\\\\ \\n\"\n",
    "\n",
    "    s += f\"& & Collision rate [\\\\%] & {np.round(data['final_collision_rate'], 2)} \\\\\\\\ \\n\"\n",
    "    \n",
    "    data[\"performed_steps\"] = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "    performed_steps = \"{:.0e}\".format(data[\"performed_steps\"])\n",
    "    s += f\"& & Total performed steps & {performed_steps} \\\\\\\\ \\n\"\n",
    "    s += \"\\\\hline\"\n",
    "\n",
    "    return s, data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Env 1 data. 6459 is an applicable run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3413,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV1:\n",
    "    for id in selected_runs:\n",
    "        s, _ = get_base_result_string_data(summary_dict, id)\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Env 2 data. 6465 is candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env2_result_string_data(summary_dict: dict, id: int) -> tuple[str, dict]:\n",
    "    s, data = get_base_result_string_data(summary_dict, id)\n",
    "    s = \"\\\\hline\".join(s.split(\"\\\\hline\")[:-1])\n",
    "\n",
    "    # Get y data.\n",
    "    y_door_passage_incorrect = get_y_data(summary_dict, id, Tag.DOOR_BAD)\n",
    "    y_door_passage_correct = get_y_data(summary_dict, id, Tag.DOOR_GOOD)\n",
    "    y_ep_length = get_y_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "    # Get x data.\n",
    "    x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "    num_datapoints = data[\"num_datapoints\"]\n",
    "\n",
    "    data[\"door_passage_quality\"] = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-num_datapoints:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-num_datapoints:]))\n",
    "        )\n",
    "    \n",
    "    data[\"door_passage_total\"] = np.mean(y_door_passage_correct[-num_datapoints:]) + (-1 * np.mean(y_door_passage_incorrect[-num_datapoints:]))\n",
    "    s += f\"& & Door passage quality & {np.round(data['door_passage_quality'], 2)} \\\\\\\\ \\n\"\n",
    "\n",
    "    data[\"final_ep_length\"] = np.mean(y_ep_length[-num_datapoints:])\n",
    "    data[\"final_ep_count\"] = 10000 / data[\"final_ep_length\"] \n",
    "\n",
    "    data[\"door_passage_per_ep\"] = data[\"door_passage_total\"] / data[\"final_ep_count\"]\n",
    "\n",
    "    s += f\"& & Door passages per episode & {np.round(data['door_passage_per_ep'], 2)} \\\\\\\\ \\n\"\n",
    "    #s += f\"& & Door passages total & {np.round(data['door_passage_total'], 3)} \\\\\\\\ \\n\"\n",
    "    s += \"\\\\hline\"\n",
    "\n",
    "    return s, data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate environment 2 result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3415,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV2:\n",
    "    for id in selected_runs:\n",
    "        s, _ = get_env2_result_string_data(summary_dict, id)\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Latex table containing sensor study results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position(x, min_val, max_val):\n",
    "    # Calculate the midpoint\n",
    "    #midpoint = (min_val + max_val) / 2.0\n",
    "    \n",
    "    # Calculate the position\n",
    "    #position = 2 * (x\"\"\" - midpoint\"\"\") / (max_val - min_val)\n",
    "    #position = 2 * (x) / (max_val - min_val)\n",
    "    position = (x - min_val) / (max_val - min_val)\n",
    "\n",
    "    if position < 0.0:\n",
    "        return 0.0\n",
    "    elif position > 1.0:\n",
    "        return 1.0\n",
    "    return position\n",
    "\n",
    "def get_colour_string(value, data: dict, tag, low_is_good = False) -> str:\n",
    "    all_values = []\n",
    "    for id in data.keys():\n",
    "        all_values.append(data[id][tag])\n",
    "\n",
    "    max_value = np.max(all_values)\n",
    "    min_value = np.min(all_values)\n",
    "\n",
    "    pos = calculate_position(value, min_value, max_value)\n",
    "\n",
    "    max_alpha_value = 80\n",
    "\n",
    "    if low_is_good:\n",
    "        #if pos <= 0:\n",
    "        #    s = f\"{{green!{np.round(pos * -max_alpha_value, 2)}}}\"\n",
    "        #else:\n",
    "        #    s = f\"{{red!{np.round(pos * max_alpha_value, 2)}}}\"\n",
    "        s = f\"{{LimeGreen!{np.round((1-pos) * max_alpha_value, 2)}}}\"\n",
    "    else:\n",
    "        #if pos <= 0:\n",
    "        #    s = f\"{{red!{np.round(pos * -max_alpha_value, 2)}}}\"\n",
    "        #else:\n",
    "        #    s = f\"{{green!{np.round(pos * max_alpha_value, 2)}}}\"\n",
    "        s = f\"{{LimeGreen!{np.round((pos) * max_alpha_value, 2)}}}\"\n",
    "\n",
    "    return f\"\\\\cellcolor{s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hline\n",
      "& \\multicolumn{9}{c}{Sensor count} \\\\\n",
      "Performance indicator & \\cellcolor{1Color!50}1 & \\cellcolor{2Color!50}2 & \\cellcolor{3Color!50}3 & \\cellcolor{4Color!50}4 & \\cellcolor{8Color!50}8 & \\cellcolor{16Color!50}16 & \\cellcolor{32Color!50}32 & \\cellcolor{64Color!50}64 & \\cellcolor{128Color!50}128 \\\\\n",
      "\\hline\n",
      "Cumulative reward & \\cellcolor{LimeGreen!0.0}-0.11 & \\cellcolor{LimeGreen!0.0}-0.11 & \\cellcolor{LimeGreen!8.98}-0.02 & \\cellcolor{LimeGreen!4.94}-0.06 & \\cellcolor{LimeGreen!15.04}0.04 & \\cellcolor{LimeGreen!66.53}0.55 & \\cellcolor{LimeGreen!71.57}0.6 & \\cellcolor{LimeGreen!73.59}0.62 & \\cellcolor{LimeGreen!79.65}0.68 \\\\\n",
      "Cumulative reward STD & \\cellcolor{LimeGreen!46.18}0.09 & \\cellcolor{LimeGreen!61.72}0.07 & \\cellcolor{LimeGreen!30.63}0.11 & \\cellcolor{LimeGreen!38.41}0.1 & \\cellcolor{LimeGreen!0.0}0.15 & \\cellcolor{LimeGreen!69.49}0.06 & \\cellcolor{LimeGreen!77.26}0.05 & \\cellcolor{LimeGreen!77.26}0.05 & \\cellcolor{LimeGreen!77.26}0.05 \\\\\n",
      "Episode length & \\cellcolor{LimeGreen!0.19}158.76 & \\cellcolor{LimeGreen!0.0}158.93 & \\cellcolor{LimeGreen!10.85}148.96 & \\cellcolor{LimeGreen!11.51}148.36 & \\cellcolor{LimeGreen!22.46}138.3 & \\cellcolor{LimeGreen!62.09}101.88 & \\cellcolor{LimeGreen!63.99}100.14 & \\cellcolor{LimeGreen!71.64}93.11 & \\cellcolor{LimeGreen!80.0}85.43 \\\\\n",
      "Target rate [\\%] & \\cellcolor{LimeGreen!5.19}37.42 & \\cellcolor{LimeGreen!0.0}33.76 & \\cellcolor{LimeGreen!9.8}40.67 & \\cellcolor{LimeGreen!7.9}39.33 & \\cellcolor{LimeGreen!18.86}47.06 & \\cellcolor{LimeGreen!68.55}82.12 & \\cellcolor{LimeGreen!75.27}86.86 & \\cellcolor{LimeGreen!73.95}85.93 & \\cellcolor{LimeGreen!80.0}90.2 \\\\\n",
      "Collision rate [\\%] & \\cellcolor{LimeGreen!0.0}0.11 & \\cellcolor{LimeGreen!80.0}0.07 & \\cellcolor{LimeGreen!80.0}0.07 & \\cellcolor{LimeGreen!17.66}0.1 & \\cellcolor{LimeGreen!59.87}0.08 & \\cellcolor{LimeGreen!59.87}0.08 & \\cellcolor{LimeGreen!59.87}0.08 & \\cellcolor{LimeGreen!80.0}0.07 & \\cellcolor{LimeGreen!59.87}0.08 \\\\\n",
      "Door passage quality & \\cellcolor{LimeGreen!24.05}0.59 & \\cellcolor{LimeGreen!73.94}0.7 & \\cellcolor{LimeGreen!33.12}0.61 & \\cellcolor{LimeGreen!37.66}0.62 & \\cellcolor{LimeGreen!1.37}0.54 & \\cellcolor{LimeGreen!73.94}0.7 & \\cellcolor{LimeGreen!73.94}0.7 & \\cellcolor{LimeGreen!73.94}0.7 & \\cellcolor{LimeGreen!78.48}0.71 \\\\\n",
      "Door passages per episode & \\cellcolor{LimeGreen!24.81}0.34 & \\cellcolor{LimeGreen!0.2}0.01 & \\cellcolor{LimeGreen!1.69}0.03 & \\cellcolor{LimeGreen!1.69}0.03 & \\cellcolor{LimeGreen!34.51}0.47 & \\cellcolor{LimeGreen!77.76}1.05 & \\cellcolor{LimeGreen!80.0}1.08 & \\cellcolor{LimeGreen!80.0}1.08 & \\cellcolor{LimeGreen!77.01}1.04 \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_SENSOR_STUDY:\n",
    "    run_data = {}\n",
    "\n",
    "    for id in selected_runs:\n",
    "        \n",
    "        _, data = get_env2_result_string_data(summary_dict, id)\n",
    "        run_data[id] = data\n",
    "\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"& \\\\multicolumn{9}{c}{Sensor count} \\\\\\\\\")\n",
    "    \n",
    "    s = f\"Performance indicator & \"\n",
    "    for id in selected_runs:\n",
    "        c = run_data[id]['sensor_count']\n",
    "        s+= f\"\\\\cellcolor{{{c}Color!50}}{c} & \"\n",
    "    s = s[:-2] + \"\\\\\\\\\"\n",
    "    print(s)\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    colour_cell = True\n",
    "    low_good = True\n",
    "    high_good = False\n",
    "\n",
    "    performance_indicators = [\n",
    "        (\"Sensors\", \"sensor_count\"),\n",
    "        (\"Cumulative reward\", \"final_cumulative_reward\", high_good),\n",
    "        (\"Cumulative reward STD\", \"final_cumulative_reward_std\", low_good),\n",
    "        (\"Episode length\", \"final_ep_length\", low_good),\n",
    "        (\"Target rate [\\\\%]\", \"target_rate\", high_good),\n",
    "        (\"Collision rate [\\\\%]\", \"final_collision_rate\", low_good),\n",
    "        (\"Door passage quality\",\"door_passage_quality\", high_good),\n",
    "        #(\"Incorrect Door passages\",\"door_passage_incorrect\"),\n",
    "        #(\"Correct Door passages\",\"door_passage_correct\")\n",
    "        (\"Door passages per episode\", 'door_passage_per_ep', high_good),\n",
    "        #(\"Door passage count\",\"door_passage_total\", high_good)\n",
    "    ]\n",
    "    for indicator in performance_indicators:\n",
    "        if indicator == (\"Sensors\", \"sensor_count\"):\n",
    "            continue\n",
    "        s = f\"{indicator[0]} & \"\n",
    "\n",
    "        for id in run_data.keys():\n",
    "            value = np.round(run_data[id][indicator[1]], 2)\n",
    "            #addition = \"\"\n",
    "            #if indicator[1] in [\"target_rate\", \"final_collision_rate\"]:\n",
    "            #    addition = \"\\\\%\"\n",
    "            s += f\"{get_colour_string(value, run_data, indicator[1], low_is_good=indicator[2])}{value} & \"\n",
    "        s = s[:-2] + \"\\\\\\\\\"\n",
    "        print(s)\n",
    "    print(\"\\\\hline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b827e17ac296308a841e2dc6cc282cd5ca71c13fcd532c5acf2c55a086afa311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
