{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1655,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import scipy.constants as sc\n",
    "from scipy.signal import savgol_filter\n",
    "#from tensorboard.backend.event_processing.event_file_loader import EventFileLoader\n",
    "\n",
    "plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1656,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    CUMULATIVE_REWARD = \"cumulative_rewards\"\n",
    "    EP_LENGTH = \"ep_length\"\n",
    "    DOOR_PASSAGE = \"passage\"\n",
    "    COLLISION_INITIAL = \"initial\"\n",
    "    COLLISION_STAY = \"stay\"\n",
    "    COLLISION_TOTAL = \"total\"\n",
    "    DOOR_GOOD = \"good_passage\"\n",
    "    DOOR_BAD = \"bad_passage\"\n",
    "\n",
    "class Plot(Enum):\n",
    "    COLLISION = 0\n",
    "    ENV1 = 1\n",
    "    ENV2 = 2\n",
    "    LINEPLOT = 3\n",
    "    RESULT_DATA_ENV1 = 4\n",
    "    RESULT_DATA_ENV2 = 5\n",
    "    DOOR = 6\n",
    "    DOOR_HIST = 7\n",
    "\n",
    "tag_colors = {\n",
    "    Tag.CUMULATIVE_REWARD: \"C1\",\n",
    "    Tag.EP_LENGTH: \"C2\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from what run to get data. Also choose the Type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1657,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_runs = True\n",
    "selected_runs: list[int] = [6465] #list(range(6445, 6454))\n",
    "plot_step = 5\n",
    "plot_all = True\n",
    "selected_tag = Tag.EP_LENGTH\n",
    "selected_plot = Plot.RESULT_DATA_ENV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1658,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get y data fro the summary dict using the ID of the training run.\n",
    "The tag specifies the selected type of data.\"\"\"\n",
    "def get_y_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    try:\n",
    "        if tag in [Tag.CUMULATIVE_REWARD, Tag.EP_LENGTH]:\n",
    "            return summary_dict[id][\"env\"][tag.value]\n",
    "        if tag in [Tag.DOOR_PASSAGE, Tag.DOOR_BAD, Tag.DOOR_GOOD]:\n",
    "            return summary_dict[id][\"door\"][tag.value]\n",
    "        if tag in [Tag.COLLISION_STAY, Tag.COLLISION_INITIAL, Tag.COLLISION_STAY]:\n",
    "            return summary_dict[id][\"collision\"][tag.value]\n",
    "    except ValueError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1659,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get x data from the summary dict. Is based on the recorded step.\"\"\"\n",
    "def get_x_data(summary_dict: dict, id: int, tag: Tag):\n",
    "    d = len(get_y_data(summary_dict, id, tag))\n",
    "    return summary_dict[id][\"steps\"][:d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1660,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter data to smooth plots.\"\"\"\n",
    "def filter_data(data):\n",
    "    try: \n",
    "        return savgol_filter(data, 15, 2)\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_arrays_to_same_length(a: np.ndarray, b:np.ndarray):\n",
    "    if len(a) != len(b):\n",
    "        if len(a) < len(b):\n",
    "            d = len(a)\n",
    "            b = b[:d]\n",
    "        else:\n",
    "            d = len(b)\n",
    "            a = a[:d]\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle with all the run data.\n",
    "summary_file_path = Path(\"C:/Users/max.muehlefeldt/Documents/GitHub/unity-machine-learning/python/basic_rl_env/summary_dict.pickle\").absolute()\n",
    "\n",
    "with open(summary_file_path, mode=\"rb\") as file:\n",
    "    summary_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "matplotlib.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.LINEPLOT:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        y = savgol_filter( get_y_data(summary_dict, id, selected_tag), 15, 2)\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag]\n",
    "            label=run_label\n",
    "            )\n",
    "    \n",
    "    \n",
    "    if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "        #axs_first.set_ylim(-2, 1.5)\n",
    "        axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_tag is Tag.EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "        axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "        axs_first.set_ylim(-1, 1)\n",
    "        axs_first.set_ylabel(\"Passage Quality\")\n",
    "        axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()   \n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_tag}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([6107, 6108, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6109, 6110, 6151, 6178, 6181, 6190, 6191, 6195, 6202, 6230, 6235, 6239, 6241, 6245, 6247, 6249, 6251, 6268, 6271, 6276, 6278, 6286, 6291, 6293, 6297, 6299, 6303, 6306, 6309, 6323, 6326, 6328, 6330, 6339, 6344, 6345, 6351, 6353, 6363, 6367, 6371, 6373, 6375, 6378, 6387, 6397, 6398, 6400, 6402, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434])"
      ]
     },
     "execution_count": 1665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dict.keys()\n",
    "#len(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "#len(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "#len(summary_dict[id][\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.COLLISION:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    fig.patch.set_alpha(0.)\n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "        run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors\"\n",
    "        #run_label += f\", no LSTM\" if id == 6108 else \", with LSTM\"\n",
    "\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        #y = savgol_filter( get_y_data(summary_dict, id, selected_tag), 15, 2)\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        #axs_first.plot(\n",
    "        #    x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag]\n",
    "        #    label=run_label\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        y_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "\n",
    "        if len(y_initial) != len(y_stay):\n",
    "            if len(y_initial) < len(y_stay):\n",
    "                d = len(y_initial)\n",
    "                y_stay = y_stay[:d]\n",
    "                x = x[:d]\n",
    "            else:\n",
    "                d = len(y_stay)\n",
    "                y_initial = y_initial[:d]\n",
    "                x = x[:d]\n",
    "        \n",
    "        y = np.vstack([y_initial, y_stay])\n",
    "\n",
    "        \n",
    "\n",
    "        axs_first.stackplot(\n",
    "            x, y,\n",
    "            labels=[\"Initial contact\", \"Continued contact\"],\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        axs_first.set_ylim(0, 200)\n",
    "    #if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "    #    #axs_first.set_ylim(-2, 1.5)\n",
    "    #    axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "    #    axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Count\")\n",
    "        axs_first.set_title(\"Collisions\")\n",
    "\n",
    "    #elif selected_tag is Tag.EP_LENGTH:\n",
    "    #    axs_first.set_ylim(0, 200)\n",
    "    #    axs_first.set_ylabel(\"Length\")\n",
    "    #    axs_first.set_title(\"Episode length\")\n",
    "    \n",
    "    #elif selected_tag is Tag.DOOR_PASSAGE:\n",
    "    #    axs_first.set_ylim(-1, 1)\n",
    "    #    axs_first.set_ylabel(\"Passage Quality\")\n",
    "    #    axs_first.set_title(\"Door passage\")\n",
    "    \n",
    "        axs_first.set_xlabel(\"Step\")\n",
    "        \n",
    "        axs_first.legend(title=f\"Run {id}\")\n",
    "        axs_first.grid()   \n",
    "\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "        plt.style.use(\"default\")\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "        fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot in [Plot.ENV1, Plot.ENV2]:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "        #sensor_count = content[id]['stats']['sensorCount']\n",
    "        sensor_count = summary_dict[id][\"file_contents\"][\"unity_config\"][\"sensorCount\"]\n",
    "        #print(sensor_count)\n",
    "        #run_label = f\"{sensor_count} sensor\" if sensor_count == 1 else f\"{sensor_count} sensors, run {id}\"\n",
    "\n",
    "        run_label = f\"Cumulutive reward\"\n",
    "        selected_tag = Tag.CUMULATIVE_REWARD\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        #y = savgol_filter( content[id][\"y_values\"][selected_tag], 15, 2)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #x = content[id][\"x_values\"][selected_tag]\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Cumulative reward\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_first.set_ylim(\n",
    "            -2, \n",
    "            1.1 if selected_plot is Plot.ENV1 else 1.6\n",
    "        )\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        reward_label = \"Reward limit = 1.0\" if selected_plot is Plot.ENV1 else  \"Reward limit = 1.5\"\n",
    "        axs_first.axhline(\n",
    "            1.0 if selected_plot is Plot.ENV1 else 1.5,\n",
    "            label=reward_label, color=\"C3\", linestyle=\"dashed\"\n",
    "        )\n",
    "\n",
    "        selected_tag = Tag.EP_LENGTH\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        axs_second = axs_first.twinx()\n",
    "        axs_second.plot(\n",
    "            x, \n",
    "            y,\n",
    "            color=tag_colors[selected_tag],\n",
    "            label=\"Episode length\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        axs_second.set_ylim(0, 200)\n",
    "        axs_second.set_ylabel(\"Episode Length\")\n",
    "        #fig.suptitle(\"Cumulative reward and episode length\")\n",
    "    \n",
    "    \"\"\"if selected_tag is Tag.CUMULATIVE_REWARD:\n",
    "        #axs_first.set_ylim(-2, 1.5)\n",
    "        axs_first.set_ylim(-2, 1.1)\n",
    "        #axs_first.axhline(1.25, label=\"Reward limit = 1.25\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.axhline(1.0, label=\"Reward limit = 1.0\", color=\"C3\", linestyle=\"dashed\")\n",
    "        axs_first.set_ylabel(\"Reward\")\n",
    "        axs_first.set_title(\"Cumulative reward\")\n",
    "\n",
    "    elif selected_tag is Tag.EP_LENGTH:\n",
    "        axs_first.set_ylim(0, 200)\n",
    "        axs_first.set_ylabel(\"Length\")\n",
    "    axs_first.set_title(\"Episode length\")\"\"\"\n",
    "    axs_first.set_title(\"Cumulative reward and episode length\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    axs_second.legend()\n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_second.grid()\n",
    "    #axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1668,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        y = -1 * y\n",
    "\n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            #color=tag_colors[selected_tag],\n",
    "            label=\"Incorrect passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "       \n",
    "        axs_first.plot(\n",
    "            x, \n",
    "            y,\n",
    "            label=\"Correct passage\",\n",
    "            linewidth=2.0,\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    \n",
    "    \n",
    "    axs_first.legend(title=f\"Run {id}\")\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.DOOR_HIST:\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    #fig = plt.figure()\n",
    "    fig.patch.set_alpha(0.)\n",
    "    \n",
    "    gs = fig.add_gridspec(ncols=1, nrows=1, figure=fig)\n",
    "\n",
    "    # First plot.\n",
    "    #selected_tag = Tag.EP_LENGTH\n",
    "    axs_first = fig.add_subplot(gs[0, 0])\n",
    "    #fig.suptitle(f\"Runs {selected_runs[0]} - {selected_runs[-1]}\", y=0.95)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    for id in selected_runs:\n",
    "              \n",
    "        selected_tag = Tag.DOOR_BAD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_bad = -1 * filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "        #y = -1 * y\n",
    "\n",
    "\n",
    "        #axs_first.hist(\n",
    "        #    #x, \n",
    "        #    y,\n",
    "        #    #color=tag_colors[selected_tag],\n",
    "        #    label=\"Incorrect passage\",\n",
    "        #    linewidth=2.0,\n",
    "        #    )\n",
    "\n",
    "        selected_tag = Tag.DOOR_GOOD\n",
    "        x = get_x_data(summary_dict, id, selected_tag)\n",
    "        y_good = filter_data(get_y_data(summary_dict, id, selected_tag))\n",
    "\n",
    "        y_bad, y_good = cut_arrays_to_same_length(y_bad, y_good)\n",
    "\n",
    "        # Use a constant bin width to make the two histograms easier to compare visually\n",
    "        #bin_width = 1\n",
    "        #bins = np.arange(np.min([y_bad, y_good]),\n",
    "                            #np.max([y_bad, y_good]) + bin_width, bin_width)\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_bad,\n",
    "            weights=-np.ones_like(y_bad),\n",
    "            #bins=bins,\n",
    "            label=\"Inorrect passage\",\n",
    "            )\n",
    "        axs_first.hist(\n",
    "            #x, \n",
    "            y_good,\n",
    "            #bins=bins,\n",
    "            label=\"Correct passage\",\n",
    "            )\n",
    "        \n",
    "    axs_first.set_title(\"Door passage\")\n",
    "    axs_first.set_xlabel(\"Step\")\n",
    "    axs_first.axhline(0, color=\"k\")\n",
    "    \n",
    "    axs_first.legend()\n",
    "    axs_first.grid()\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"{selected_run}_all.pdf\")\n",
    "    fig.savefig(f\"{selected_runs[0]}-{selected_runs[-1]}_{selected_plot}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9496"
      ]
     },
     "execution_count": 1670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result data\n",
    "To highlight the final performance of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV1:\n",
    "    for id in selected_runs:\n",
    "        y_collision_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_collision_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "        y_cumulative_reward = filter_data(get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD))\n",
    "        y_ep_length = filter_data(get_y_data(summary_dict, id, Tag.EP_LENGTH))\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        print(f\"Episode length & {np.round(final_ep_length, 3)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_quality(x, y):\n",
    "    return x / (x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward & 0.822 \\\\\n",
      "Cumulative reward STD & 0.019 \\\\\n",
      "Episode length & 63.28 \\\\\n",
      "Collision rate & 0.081\\% \\\\\n",
      "Performed steps & 100000000 \\\\\n",
      "Door passage quality $[-1, 1]$ & 0.797 \\\\\n"
     ]
    }
   ],
   "source": [
    "if selected_plot is Plot.RESULT_DATA_ENV2:\n",
    "    for id in selected_runs:\n",
    "        y_collision_initial = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_INITIAL))\n",
    "        y_collision_stay = filter_data(get_y_data(summary_dict, id, Tag.COLLISION_STAY))\n",
    "        y_cumulative_reward = filter_data(get_y_data(summary_dict, id, Tag.CUMULATIVE_REWARD))\n",
    "        y_ep_length = filter_data(get_y_data(summary_dict, id, Tag.EP_LENGTH))\n",
    "        y_door_passage_incorrect = filter_data(get_y_data(summary_dict, id, Tag.DOOR_BAD))\n",
    "        y_door_passage_correct = filter_data(get_y_data(summary_dict, id, Tag.DOOR_GOOD))\n",
    "\n",
    "        x_steps = get_x_data(summary_dict, id, Tag.EP_LENGTH)\n",
    "\n",
    "        # Episode length.\n",
    "        #final_cumulative_reward = np.mean(y_cumulative_reward[-10:])\n",
    "        final_cumulative_reward = summary_dict[selected_runs[0]][\"final_mean_reward\"]\n",
    "        print(f\"Cumulative reward & {np.round(final_cumulative_reward, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_cumulative_reward_std = summary_dict[selected_runs[0]][\"final_std_reward\"]\n",
    "        print(f\"Cumulative reward STD & {np.round(final_cumulative_reward_std, 3)} \\\\\\\\\")\n",
    "\n",
    "        final_ep_length = np.mean(y_ep_length[-10:])\n",
    "        print(f\"Episode length & {np.round(final_ep_length, 2)} \\\\\\\\\")\n",
    "\n",
    "        #final_collision_rate = 100 - (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        final_collision_rate = (np.mean(y_initial[-10:]) / 10000) * 100\n",
    "        #print(100 -  / 10000 * 100)\n",
    "        print(f\"Collision rate & {np.round(final_collision_rate, 3)}\\% \\\\\\\\\")\n",
    "\n",
    "        performed_steps = summary_dict[selected_runs[0]][\"steps\"][-1]\n",
    "        print(f\"Performed steps & {performed_steps} \\\\\\\\\")\n",
    "\n",
    "        door_passage_quality = get_passage_quality(\n",
    "            np.mean(y_door_passage_correct[-10:]),\n",
    "            (-1* np.mean(y_door_passage_incorrect[-10:])))\n",
    "        print(f\"Door passage quality $[-1, 1]$ & {np.round(door_passage_quality, 3)} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7968900373424694"
      ]
     },
     "execution_count": 1674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = np.mean(y_door_passage_correct[-10:])\n",
    "\n",
    "bad = -1* np.mean(y_door_passage_incorrect[-10:])\n",
    "\n",
    "(good ) / (good + bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.34479638009052"
      ]
     },
     "execution_count": 1675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_door_passage_incorrect[-10:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b827e17ac296308a841e2dc6cc282cd5ca71c13fcd532c5acf2c55a086afa311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
