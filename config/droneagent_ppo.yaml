behaviors:
  DroneAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512

      # (default = 10240 for PPO and 50000 for SAC)
      # PPO: Number of experiences to collect before updating the policy model.
      # Corresponds to how many experiences should be collected before we do
      # any learning or updating of the model.
      # This should be multiple times larger than batch_size.
      # Typically a larger buffer_size corresponds to more stable training updates.
      # SAC: The max size of the experience buffer - on the order of thousands of times
      # longer than your episodes, so that SAC can learn from old
      # as well as new experiences.
      buffer_size: 10240

      # (default = 3e-4)
      # Initial learning rate for gradient descent.
      # Corresponds to the strength of each gradient descent update
      # step. This should typically be decreased if training is unstable,
      # and the reward does not consistently increase. 
      learning_rate: 1.0e-4

      # (default = 5.0e-3)
      # Strength of the entropy regularization, which makes the policy "more random."
      # This ensures that agents properly explore the action space during training.
      # Increasing this will ensure more random actions are taken.
      # This should be adjusted such that the entropy (measurable from TensorBoard)
      # slowly decreases alongside increases in reward.
      # If entropy drops too quickly, increase beta.
      # If entropy drops too slowly, decrease beta.
      # Typical range: 1e-4 - 1e-2
      #beta: 5.0e-4
      beta: 1.0e-2

      # (default = learning_rate_schedule) Determines how beta changes over time.
      # linear decays beta linearly, reaching 0 at max_steps, while constant keeps beta
      # constant for the entire training run. If not explicitly set,
      # the default beta schedule will be set to
      # hyperparameters -> learning_rate_schedule.
      beta_schedule: constant

      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      
      epsilon_schedule: linear
    network_settings:
      normalize: false

      # (default = 128)
      # Number of units in the hidden layers of the neural network.
      # Correspond to how many units are in each fully connected layer of the neural network.
      # For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small.
      # For problems where the action is a very complex interaction between the observation variables, this should be larger.
      # Typical range: 32 - 512
      # was 512
      hidden_units: 256

      # (default = 2)
      # The number of hidden layers in the neural network.
      # Corresponds to how many hidden layers are present after the observation input, or after the CNN encoding of the visual observation.
      # For simple problems, fewer layers are likely to train faster and more efficiently.
      # More layers may be necessary for more complex control problems.
      # Typical range: 1 - 3
      num_layers: 2

      memory:
        # (default = 128)
        # Size of the memory an agent must keep. In order to use a LSTM,
        # training requires a sequence of experiences instead of single experiences.
        # Corresponds to the size of the array of floating point numbers used to store
        # the hidden state of the recurrent neural network of the policy.
        # This value must be a multiple of 2, and should scale with the amount of
        # information you expect the agent will need to remember
        # in order to successfully complete the task.
        # Typical range: 32 - 256
        memory_size: 128

        # (default = 64)
        # Defines how long the sequences of experiences must be while training.
        # Note that if this number is too small, the agent will not be able
        # to remember things over longer periods of time.
        # If this number is too large, the neural network will take longer to train.
        # Typical range: 4 - 128
        sequence_length: 64

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 10e6
    time_horizon: 64
    summary_freq: 10000