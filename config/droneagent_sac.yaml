behaviors:
  DroneAgent:
    # (default = ppo) The type of trainer to use: ppo, sac, or poca.
    trainer_type: sac

    hyperparameters:
      #
      # Common Trainer Configurations.
      #

      # Number of experiences in each iteration of gradient descent.
      # This should always be multiple times smaller than buffer_size.
      # If you are using continuous actions, this value should be large (on the order 
      # of 1000s). If you are using only discrete actions, this value should 
      # be smaller (on the order of 10s).
      # Typical range: (Continuous - PPO): 512 - 5120;
      # (Continuous - SAC): 128 - 1024; (Discrete, PPO & SAC): 32 - 512.
      batch_size: 128

      # (default = 10240 for PPO and 50000 for SAC)
      # PPO: Number of experiences to collect before updating the policy model.
      # Corresponds to how many experiences should be collected before we do
      # any learning or updating of the model.
      # This should be multiple times larger than batch_size.
      # Typically a larger buffer_size corresponds to more stable training updates.
      # SAC: The max size of the experience buffer - on the order of thousands of times
      # longer than your episodes, so that SAC can learn from old
      # as well as new experiences.
      buffer_size: 400000

      # (default = 3e-4)
      # Initial learning rate for gradient descent.
      # Corresponds to the strength of each gradient descent update
      # step. This should typically be decreased if training is unstable,
      # and the reward does not consistently increase. 
      learning_rate: 1.0e-5

      # (default = linear for PPO and constant for SAC)
      # Determines how learning rate changes over time.
      # For PPO, we recommend decaying learning rate until max_steps so learning
      # converges more stably. However, for some cases (e.g. training for an unknown
      # amount of time) this feature can be disabled.
      # For SAC, we recommend holding learning rate constant so that the agent can
      # continue to learn until its Q function converges naturally.
      # linear decays the learning_rate linearly, reaching 0 at max_steps, while
      # constant keeps the learning rate constant for the entire training run.
      learning_rate_schedule: constant
      
      #
      # SAC-specific Configurations.
      #

      # (default = 0)
      # Number of experiences to collect into the buffer before updating the policy
      # model. As the untrained policy is fairly random, pre-filling the buffer with
      # random actions is useful for exploration.
      # Typically, at least several episodes of experiences should be pre-filled.
      # Typical range: 1000 - 10000
      buffer_init_steps: 1000

      # (default = 1.0)
      # How much the agent should explore in the beginning of training.
      # Corresponds to the initial entropy coefficient set at the beginning of training.
      # In SAC, the agent is incentivized to make its actions entropic to facilitate
      # better exploration. The entropy coefficient weighs the true reward with a bonus
      # entropy reward. The entropy coefficient is automatically adjusted to a preset
      # target entropy, so the init_entcoef only corresponds to the starting value of
      # the entropy bonus. Increase init_entcoef to explore more in the beginning,
      # decrease to converge to a solution faster.
      # Typical range: (Continuous): 0.5 - 1.0; (Discrete): 0.05 - 0.5
      init_entcoef: 0.5

      save_replay_buffer: false

      # (default = 0.005)
      # How aggressively to update the target network used for bootstrapping value
      # estimation in SAC. Corresponds to the magnitude of the target Q update during
      # the SAC model update. In SAC, there are two neural networks: the target and the
      # policy. The target network is used to bootstrap the policy's estimate of the
      # future rewards at a given state, and is fixed while the policy is being updated.
      # This target is then slowly updated according to tau. Typically, this value
      # should be left at 0.005. For simple problems, increasing tau to 0.01 might
      # reduce the time it takes to learn, at the cost of stability.
      # Typical range: 0.005 - 0.01
      tau: 0.005
      
      steps_per_update: 10
      #reward_signal_num_update: steps_per_update


    network_settings:
      normalize: false

      # (default = 128)
      # Number of units in the hidden layers of the neural network.
      # Correspond to how many units are in each fully connected layer of the neural network.
      # For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small.
      # For problems where the action is a very complex interaction between the observation variables, this should be larger.
      # Typical range: 32 - 512
      hidden_units: 128

      # (default = 2)
      # The number of hidden layers in the neural network.
      # Corresponds to how many hidden layers are present after the observation input, or after the CNN encoding of the visual observation.
      # For simple problems, fewer layers are likely to train faster and more efficiently.
      # More layers may be necessary for more complex control problems.
      # Typical range: 1 - 3
      num_layers: 2

      memory:
        # (default = 128)
        # Size of the memory an agent must keep. In order to use a LSTM,
        # training requires a sequence of experiences instead of single experiences.
        # Corresponds to the size of the array of floating point numbers used to store
        # the hidden state of the recurrent neural network of the policy.
        # This value must be a multiple of 2, and should scale with the amount of
        # information you expect the agent will need to remember
        # in order to successfully complete the task.
        # Typical range: 32 - 256
        memory_size: 128

        # (default = 64)
        # Defines how long the sequences of experiences must be while training.
        # Note that if this number is too small, the agent will not be able
        # to remember things over longer periods of time.
        # If this number is too large, the neural network will take longer to train.
        # Typical range: 4 - 128
        sequence_length: 64

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    
    max_steps: 10e6
    time_horizon: 64
    summary_freq: 10000